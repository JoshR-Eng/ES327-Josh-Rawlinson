% !TEX root =  ../Report.tex

\chapter{Literature Review}
\label{sec: Lit Review}


%==============================================================
%                        section 1. 
%        
\section{Battery State Estimation: Tasks and Challenges}
%==============================================================


%==============================================================
%                       section 2. 
%        
\section{State-of-Charge \& State-of-Health Estimation}
%==============================================================
\subsection{Model-based Approach}


\subsection{Machine-Learning Based Approach}


\subsection{Real-world Data and Operational Constraints}



%==============================================================
%                       section 3. 
%         
\section{Temporal Deep Learning Architectures}
%==============================================================
\subsection{Machine Learning for Sequence Modelling}


\subsection{Recurrent Neural Networks}


\subsection{Convolutional and Temporal Convolution Networks}



%==============================================================
%                       section 4. 
%                      
\section{Edge and On-Device Intelligence}
%==============================================================

    % 4.1
\subsection{Distributing Cloud AI to the Edge}
    % |
The conventional paradigm of Cloud-centric \ac{ai} relies heavily on centralised processing, while offering virtually unlimited compute, it introduces latency, bandwidth bottlenecks and transmission-energy overheads that are fundamentally infeasible for real-time, safety-critical \ac{iot} applications like \ac{bms} \cite{surianarayanan_survey_2023, rosendo_distributed_2022}. Consequently, the computing landscape is migrating towards an "Edge-to-Cloud Continuum" \cite{rosendo_distributed_2022}, pushing intelligence next to the data source to ensure microsecond-level responsiveness and data privacy \cite{gill_edge_2024, wang_empowering_2025}. \\
% [Could put an image of Edge to Cloud computing continuum architecture]
However, this decentralisation exposes a critical concern in sustainability. Historically, deep learning literature has prioritised "Red \ac{ai}" - an approach that chases \ac{sota} accuracy through exponentially larger models and datasets, resulting in computing demands that outpace Moore's Law \cite{salehi_data-centric_2024, barbierato_toward_2024}. Deploying these heavily parametrised, energy-intensive models to resource-constrained edge devices threatens to negate the energy efficiency the models are designed to achieve. \\
This development has catalysed the concept of "Green \ac{ai}", which shifts the optimisation objective from pure accuracy maximisation to multi-objective sustainability \cite{philipo_sustainable_2025}. Barbierato and Gatti \cite{barbierato_toward_2024} argue that a Red vs. Green \ac{ai} trade-off is inevitable, suggesting that targetting a "good enough" accuracy with superior energy and latency metrics is a more rigorous engineering objective for edge application. In applications to batteries, recent studes by Kumar et al. \cite{kumar_towards_2025, kumar_towards_2026} validate this approach. By data-focused Green \ac{ai} techniques (e.g. algorithmic data filtering), they reduce \ac{soh} computaional time and energy consumption by over 60\% while preserving predictive fidelity. \\
% [Could put image of "Red AI vs. Green AI computational cost trade off curve"]
Yet, while works like Kumar et al. optimise the \textit{algorithmic} and \textit{data} components, a fundamental gap remains. Mao et al. \cite{mao_green_2024} explicitly warn that software-level benefits are heavily dictated by the underlying hardware execution. True "Green Edge \ac{ai}" requires extensive hardware-software co-design \cite{surianarayanan_survey_2023, mao_green_2024}. Therefore, evaluating the sustainability of \ac{soh} models requires empirical profiling of hardware-specific implementations rather than relying on abstract algorithmic performance.



% Version 2.0
% The conventional paradigm of Cloud-centric \ac{ai}, while offering virtually ulimited computational resources, introduces latency and connectivity dependencies unsuitable for safety-critical application \cite{rosendo_distributed_2022}. Such as \ac{bms}, the continuos transmission of high-frequency data to centralised servers incurs extensive bandwidth costs and privacy vulnerability. Consequently, there's a rapid migration towards the "Edge-to-Cloud Continuum" \cite{rosendo_distributed_2022}, where digital intelligence is decentralised and inference occurs directly on-device \cite{wang_empowering_2025, gill_edge_2024}. \\
% As deep learning models continue to grow in complexity, their energy consumption threatens to offset the efficiency gains they are designed to provide \cite{philipo_sustainable_2025}. This has lead to the emergence of "Green \ac{ai}" \cite{mao_green_2024}, a re-thinking on AI to minimise the carbon footprint and energy expenditure of machine learning workloads. Within the battery domain



% Version 1.0
% The present paradigm of Cloud-centric \ac{ai}, while offering virtually unlimited computational resources, introduces latency and connectivity dependencies unsuitable for safety-critical applications such as \ac{bms} \cite{rosendo_distributed_2022}. As discussed by Rosendo et al., the continous transmission of high-frequency data to a central server incurs significant bandwidth costs and privacy risks - the growing number of interconnected IoT devices elevates these concerns. \\
% Consequently, the industry is migrating towards "Edge Intelligence" where inference occurs directly at the source of the data. This shift, however, imposes strict constraints on the computational availability and resource consumption, necessitating a departure from general-purpose computing towards specialised, energy-efficient hardware architectures \cite{mao_green_2024}.\\
% Crucially, as noted by Barbierato \& Gatti \cite{barbierato_toward_2024}, current literature often reports hardware efficiency using theoretical metrics ( e.g. \ac{flops}) rather then empirical energy measurements. This abstraction obscures critical physical effects such a memory bandwidth bottlenecks, quantisation noise, and static power overheads - factors which this research seeks to quantify explicitly


    % 4.2
\subsection{TinyML to GPU-Class Edge Devices}
    % |
Achieving this empirical hardware-software co-design requires a deployment platform that balances energy constraints against the computational demands of \ac{dl} temporal modelling. The embedded hardware spectrum spans from ultra-low power ac{tinyml} microcontrollers to "GPU-class" edge accelerators \cite{wang_empowering_2025}. \\
At the lower end, \ac{tinyml} solutions, such as the \textit{TinyOL} framework deployed on ARM Cortex-M architecture \cite{ren_tinyol_2021}, operate within ultra-low power ($<0.1\si{\watt}$). While Kim and Ben-Othman \cite{kim_eco-friendly_2023} highlight the utility of low-resources devices, these scalar processing \ac{mcu} have limitations. They lack the parallel memory bandwidth for \ac{dl} temporal models and struggle with the complex, on-device adaptations required to combat battery concept drift over time \cite{ren_tinyol_2021}.\\
Consequently, enabling multi-cell \ac{soh} estimation using modern 'Congnitive Edge Computing' architectures \cite{wang_empowering_2025} demands dedicated matrix-multiplication acceleration. Between lower-power \ac{mcu}'s and Cloud-class compute, the NVIDIA Jetson Orin Nano (4GB) emerges as the optimal compromise. Built on the NVIDIA Ampere \ac{soc} architecture, the Orin Nano integrates an ARM Cortex-A78AE \ac{cpu} with a 512-core \ac{gpu} and 16 dedicated Tensor Cores \cite{nvidia_orin_nano_datasheet}.\\
% [Could add "Image of NVIDIA Jetson Orin Nano SoC hardware architecture diagram"]
The Tensor Cores are explicitly designed to accelerate spare \ac{int8} precision formats, delivering up to 20 \ac{tops} of \ac{ai} performance \cite{nvidia_orin_nano_datasheet}. Additionally, the Jetson power architecture enables dynamic hardware constraints via the \texttt{nvpmodel} utility, allowing the module to be restricted to specific power modes (e.g. $5\si{\watt}$ to $10\si{\watt}$) by disabling cores and scaling clock frequencies \cite{noauthor_power_2023, noauthor_jetson-stats_nodate}. \\
By deploying on this platform, the research shifts from theoretical estimates to physical measurement. The core objectives becomes identifying the optimal configuration for targetted hardware, adapting model architecture and power budgets to achieve "good enough" SOH accuracy at the lowest emperical energy cost.



%        Version 1
% The hardware landscape for edge intelligence spans a wide spectrum, from ultra-low power \ac{mcu} running \ac{tinyml} to high-performance 'GPU-equipped' accelerators \cite{wang_empowering_2025}. \\
% \ac{tinyml} solutions, such as the online learning framework proposed by Ren et al. \cite{ren_tinyol_2021}, offer extreme energy efficiency, operating within a milliwatt power envelope ($\sim$ \si{\milli \watt} ). However these typically rely on sequential processing (e.g. ARM Cortex-M4) which severely limits their throughput. While sufficient for estimating the \ac{soc} of a single cell, they lack the parallel compute density required to process high-dimensional temporal data for a multi-cell battery pack, along with additional overheads, in real-time.\\
% In contrast, the NVIDIA Jetson Orin series represents a 'High-Performance Edge' class designed to bridge this scalability gap. Built on the NVIDIA Ampere architecture, the Orin integrates specialised 'Tensor Cores' optimised for \ac{int8} matrix operations, delivering up to 40 \ac{tops} within a configurable \SI{5}{\watt}--\SI{15}{\watt} envelope (model dependant) \cite{nvidia_orin_nano_datasheet}. This eases the computational constraints, enabling the deployment of complex \ac{dl} models (such as \ac{tcn}s) that would be infeasible on \ac{mcu} hardware. The critical research question, therefore, is whether the increase in power consumption is justified by a proportional gain in estimation accuracy and latency reduction.


    % 4.3
\subsection{Precision Trade-offs: FP32, INT8, and the cost of accuracy}
    % |
The deployment of \ac{ml} to edge architectures provides critical data security and latency benefits, but is fundamentally limited by memory-constraint. By default, \ac{dl} models are trained using \ac{fp32} formats however, a strong consensus in the literature indicates that \ac{fp32} is heavily over-parametrised for inference tasks \cite{gholami_survey_2021, hubara_quantized_2017}. \\
Consequently, quantisation of floating-point values to lower-bit integer representations is a necessary precondition for edge deployment \cite{tong_enhancing_2026, gholami_survey_2021}. Among the available formats, \ac{int8} has emerged as the industry standard, the optimal balance between memory efficiency, accuracy retention and hardware acceleration \cite{hasanpour_survey_2026}. While recent studies have explored 8-bit floating-point formats, Baalen et al. \cite{baalen_fp8_2023} shows that \ac{int8} inference remains 2 to 8 times more efficient than its \ac{fp32} counterpart, and significantly outperforms FP8 in both area and energy usage.\\
The methodology of quantisation itself heavily dictates hardware performance. Hasanpour et al. note that uniform affine quantization remains the dominant mapping while non-linear or logarithmic quantisation might theoretically better match data distributions, they suffer from complex implementation and poor hardware support \cite{hasanpour_survey_2026, gholami_survey_2021}. Additionally, the timing of quantisation plays a significant role in accuracy retention. \ac{ptq} is computationally lightweight but frequently introduces unacceptable accuracy degradation \cite{hasanpour_survey_2026, tong_enhancing_2026}. Alternatively, \ac{qat} allows the model to adapt to quantisation noise during the training phase enabling accuracy retention, though adapting this dynamically on edge devices remains a security and computational challenge \cite{tong_enhancing_2026}.\\
Implementation of these quantisation schemes to battery state estimation introduces significant vulnerabilities as \ac{soh} degradation is a subtle, long-term time-series trend. If the degradation features fall outside the resolution of a uniform \ac{int8} quantisation bin, the model risks catastrophic information loss. Therefore, systematically evaluating how temporal models navigate the trade-off between \ac{ptq}/\ac{qat} accuracy loss and \ac{fp32}/\ac{fp16}/\ac{int8} energy efficiency forms a primary objective of this study.


    % 4.4
\subsection{Power, Energy and Resource Measurement on NVIDIA Jetson}
    % |
Transitioning towards energy-aware edge computing requires robust empirical profiling however, accurately quantifying the power consumption of embedded hardware remains a methodological challenge \cite{shalavi_energy_2022}. Software-based power estimation, such as \ac{pmc} models, have been application specific and require extensive feature engineering to achieve sufficient accuracy \cite{song_simplified_2013}. Alternatively, relying on on-chip hardware sensors introduces uncertainty and reliability concerns. As Fahad et al. \cite{fahad_comparative_2019} demonstrates, dynamic energy profiles generated by internal \ac{cpu}/\ac{gpu} sensors can deviate from external ground-truth metrics by 8\% to 73\%, indeterminate whether the primary energy cost is arithmetic computation or memory transfers. \\
Profiling on the NVIDIA Jetson Orin introduces platform-specific complexities, Unlike discrete data-centre \ac{gpu}s, the Jetson not support the standard NVIDIA Management Library (NVML) \cite{noauthor_nvidia_nodate}. Power telemetry must instead be polled via \texttt{sysfs} nodes or the \texttt{tegrastats} utility \cite{aslan_study_2022, noauthor_power_2023}. Yet, these raw sensor readings are noisy, exhibiting non-physical, jagged spikes due to low temporal resolution and internal hardware quantisation \cite{aslan_study_2022}. This necessitates signal processing of the raw sensor data, such as moving average filter, to extract realistic dynamic profiles. \\
Further research by Shalavi et al. \cite{shalavi_energy_2022}, conducts a rigorous external-meter validation across the Jetson hardware family, unrevealing that built-in sensors (e.g. INA3221) deviate from the ground-truth by up to 50\% due to unmonitored board-level losses. To resolve this, they derived linear regression models to calibrate internal readings (e.g. adjusting Orin readings via. $1.02x + 3115.39$ \si{\milli \watt}). By applying this calibration technique, measurement error is successfully reduced to $\pm 3\%$.\\
For deployment of \ac{nn}s on Jetson hardware, Holly et al. \cite{holly_profiling_2020} demonstrated that power draw remains relatively constant across varying model layers. Instead, the variance in total energy consumption is driven almost entirely by latency but manipulation of hardware constraints via \ac{dvfs} and power modes yields non-trivial optimums. Maximising \ac{gpu} frequency severely increases power but drastically cuts latency, creating complex accuracy-latency-energy trade-offs \cite{holly_profiling_2020}.\\
Hence, this research leverages calibrated \texttt{tegrastats} metrics to systematically evaluate how different temporal \ac{soh} architectures and quantisation precisions interact with Jetson Orin power modes, identifying the optimal configuration for real-world \ac{bms} deployment.




%==============================================================
%                       section 5. 
%                      
\section{Summary of Gaps and Research Questions}
%==============================================================
