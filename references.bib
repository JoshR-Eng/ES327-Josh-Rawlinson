@manual{nvidia_orin_nano_datasheet,
    title        = {NVIDIA Jetson Orin Nano Series Data Sheet},
    author       = {{NVIDIA Corporation}},
    number       = {DS-11105-001\_v1.1},
    year         = {2023},
    organization = {NVIDIA},
    note         = {Accessed: 2026-02-13}
}

@article{rosendo_distributed_2022,
    title = {Distributed intelligence on the {Edge}-to-{Cloud} {Continuum}: {A} systematic literature review},
    volume = {166},
    issn = {0743-7315},
    shorttitle = {Distributed intelligence on the {Edge}-to-{Cloud} {Continuum}},
    url = {https://www.sciencedirect.com/science/article/pii/S0743731522000843},
    doi = {10.1016/j.jpdc.2022.04.004},
    abstract = {The explosion of data volumes generated by an increasing number of applications is strongly impacting the evolution of distributed digital infrastructures for data analytics and machine learning (ML). While data analytics used to be mainly performed on cloud infrastructures, the rapid development of IoT infrastructures and the requirements for low-latency, secure processing has motivated the development of edge analytics. Today, to balance various trade-offs, ML-based analytics tends to increasingly leverage an interconnected ecosystem that allows complex applications to be executed on hybrid infrastructures where IoT Edge devices are interconnected to Cloud/HPC systems in what is called the Computing Continuum, the Digital Continuum, or the Transcontinuum. Enabling learning-based analytics on such complex infrastructures is challenging. The large scale and optimized deployment of learning-based workflows across the Edge-to-Cloud Continuum requires extensive and reproducible experimental analysis of the application execution on representative testbeds. This is necessary to help understand the performance trade-offs that result from combining a variety of learning paradigms and supportive frameworks. A thorough experimental analysis requires the assessment of the impact of multiple factors, such as: model accuracy, training time, network overhead, energy consumption, processing latency, among others. This review aims at providing a comprehensive vision of the main state-of-the-art libraries and frameworks for machine learning and data analytics available today. It describes the main learning paradigms enabling learning-based analytics on the Edge-to-Cloud Continuum. The main simulation, emulation, deployment systems, and testbeds for experimental research on the Edge-to-Cloud Continuum available today are also surveyed. Furthermore, we analyze how the selected systems provide support for experiment reproducibility. We conclude our review with a detailed discussion of relevant open research challenges and of future directions in this domain such as: holistic understanding of performance; performance optimization of applications; efficient deployment of Artificial Intelligence (AI) workflows on highly heterogeneous infrastructures; and reproducible analysis of experiments on the Computing Continuum.},
    urldate = {2026-02-07},
    journal = {Journal of Parallel and Distributed Computing},
    author = {Rosendo, Daniel and Costan, Alexandru and Valduriez, Patrick and Antoniu, Gabriel},
    month = aug,
    year = {2022},
    note = {109 citations (Semantic Scholar/DOI) [2026-02-07]
TLDR: This review aims at providing a comprehensive vision of the main state-of-the-art libraries and frameworks for machine learning and data analytics available today and describes the main learning paradigms enabling learning-based analytics on the Edge-to-Cloud Continuum.},
    keywords = {Big Data Analytics, Computing Continuum, Distributed intelligence, Edge computing, Reproducibility},
    pages = {71--94},
}
@article{mao_green_2024,
    title = {Green {Edge} {AI}: {A} {Contemporary} {Survey}},
    volume = {112},
    issn = {1558-2256},
    shorttitle = {Green {Edge} {AI}},
    url = {https://ieeexplore.ieee.org/document/10637271},
    doi = {10.1109/JPROC.2024.3437365},
    abstract = {Artificial intelligence (AI) technologies have emerged as pivotal enablers across a multitude of industries, including consumer electronics, healthcare, and manufacturing, largely due to their significant resurgence over the past decade. The transformative power of AI is primarily derived from the utilization of deep neural networks (DNNs), which require extensive data for training and substantial computational resources for processing. Consequently, DNN models are typically trained and deployed on resource-rich cloud servers. However, due to potential latency issues associated with cloud communications, deep learning (DL) workflows (e.g., DNN training and inference) are increasingly being transitioned to wireless edge networks in proximity to end-user devices (EUDs). This shift is designed to support latency-sensitive applications and has given rise to a new paradigm of edge AI, which will play a critical role in upcoming sixth-generation (6G) networks to support ubiquitous AI applications. Despite its considerable potential, edge AI faces substantial challenges, mostly due to the dichotomy between the resource limitations of wireless edge networks and the resource-intensive nature of DL. Specifically, the acquisition of large-scale data, as well as the training and inference processes of DNNs, can rapidly deplete the battery energy of EUDs. This necessitates an energy-conscious approach to edge AI to ensure both optimal and sustainable performance. In this article, we present a contemporary survey on green edge AI. We commence by analyzing the principal energy consumption components of edge AI systems to identify the fundamental design principles of green edge AI. Guided by these principles, we then explore energy-efficient design methodologies for the three critical tasks in edge AI systems, including training data acquisition, edge training, and edge inference. Finally, we underscore potential future research directions to further enhance the energy efficiency (EE) of edge AI.},
    number = {7},
    urldate = {2026-02-10},
    journal = {Proceedings of the IEEE},
    author = {Mao, Yuyi and Yu, Xianghao and Huang, Kaibin and Angela Zhang, Ying-Jun and Zhang, Jun},
    month = jul,
    year = {2024},
    note = {62 citations (Semantic Scholar/DOI) [2026-02-10]
TLDR: A contemporary survey on green edge AI is presented, analyzing the principal energy consumption components of edge AI systems to identify the fundamental design principles of green edge AI, and exploring energy-efficient design methodologies for the three critical tasks in edge AI systems, including training data acquisition, edge training, and edge inference.},
    keywords = {6G mobile communication, Artificial intelligence, Cloud computing, Data acquisition, Edge AI, Edge computing, Energy consumption, Energy efficiency, Federated learning, Surveys, Training, Wireless networks, edge artificial intelligence (AI), edge inference, energy efficiency (EE), federated learning (FL), green AI, mobile edge computing (MEC), sixth-generation (6G) wireless networks},
    pages = {880--911},
}
@article{barbierato_toward_2024,
    title = {Toward {Green} {AI}: {A} {Methodological} {Survey} of the {Scientific} {Literature}},
    volume = {12},
    issn = {2169-3536},
    shorttitle = {Toward {Green} {AI}},
    url = {https://ieeexplore.ieee.org/document/10418137},
    doi = {10.1109/ACCESS.2024.3360705},
    abstract = {The pervasive deployment of Deep Learning models has recently prompted apprehensions regarding their ecological footprint, owing to the exorbitant levels of energy consumption necessitated by the training and inference processes. The term “Red AI” is employed to denote artificial intelligence (AI) models that undergo training using resource-intensive methodologies on very large datasets. This practice can engender substantial energy usage and emissions of carbon, thereby opposing “Green AI. ” The latter concept alludes to AI models designed for similar efficiency and reduced environmental impact. This objective is realized through the utilization of smaller datasets, less computationally intensive training techniques, or sustainable energy resources. While Red AI prioritizes accuracy and performance, Green AI emphasizes efficiency and sustainability. Given that both paradigms exhibit advantages and limitations, the debates around the topics have burgeoned in the scientific arena, delving into novel algorithms, hardware innovations, and improved data utilization techniques aimed at mitigating the ecological consequences of intricate applications such as GPT and BERT. Nevertheless, due to the relative novelty of this debate, not much effort has been dedicated yet to contextualizing the essence of Red AI and the prospects of Green AI in a coherent framework. Within this context, the present work contributes by meticulously delineating both domains through a multifaceted analysis of their causes and ramifications, described from the points of computer architectures, data structures, and algorithms. Additionally, the study reviews notable instances of study cases based on complex Red AI models. The primary contribution of this article encompasses a comprehensive survey of Red and Green AI, stemming from a selection of the literature performed by the authors, subsequently organized into distinct clusters. These clusters encompass i) articles that qualitatively or quantitatively address the issue of Red AI, identifying Green AI as a plausible remedy, ii) articles offering insights into the environmental impact associated with the deployment of extensive Deep Learning models, and iii) articles introducing the techniques underpinning Green AI, aiming at mitigating the cost of Red AI. The outcome emerging from the analysis performed by this work consists of a compromise between sustainability in contrast to the performance of AI tools. Unless the complex training and inference procedures of software models mitigate their environmental impact, it will be necessary to decrease the level of accuracy of production systems, inevitably conflicting with the objective of the major AI vendors. The outcomes of this work would be beneficial to scholars pursuing intricate Deep Learning architectures in scientific research, as well as AI enterprises struggling with the protracted training demands of commercial products within the realms of Computer Vision and Natural Language Processing.},
    urldate = {2026-02-10},
    journal = {IEEE Access},
    author = {Barbierato, Enrico and Gatti, Alice},
    year = {2024},
    note = {30 citations (Semantic Scholar/DOI) [2026-02-10]
TLDR: A comprehensive survey of Red and Green AI is undertaken, meticulously delineating both domains through a multifaceted analysis of their causes and ramifications, described from the points of computer architectures, data structures, and algorithms.},
    keywords = {Artificial intelligence, Biological system modeling, Computational modeling, Computer architecture, Environmental monitoring, Green AI, Green products, Surveys, Training, environmental impact, red AI, survey},
    pages = {23989--24013},
}
@article{wang_empowering_2025,
    title = {Empowering {Edge} {Intelligence}: {A} {Comprehensive} {Survey} on {On}-{Device} {AI} {Models}},
    volume = {57},
    issn = {0360-0300},
    shorttitle = {Empowering {Edge} {Intelligence}},
    url = {https://dl.acm.org/doi/10.1145/3724420},
    doi = {10.1145/3724420},
    abstract = {The rapid advancement of artificial intelligence (AI) technologies has led to an increasing deployment of AI models on edge and terminal devices, driven by the proliferation of the Internet of Things (IoT) and the need for real-time data processing. This survey comprehensively explores the current state, technical challenges, and future trends of on-device AI models. We define on-device AI models as those designed to perform local data processing and inference, emphasizing their characteristics such as real-time performance, resource constraints, and enhanced data privacy. The survey is structured around key themes, including the fundamental concepts of AI models, application scenarios across various domains, and technical challenges faced in edge environments. We also discuss optimization and implementation strategies, such as data preprocessing, model compression, and hardware acceleration, which are essential for effective deployment. Furthermore, we examine the impact of emerging technologies, including edge computing and foundation models, on the evolution of on-device AI models. By providing a structured overview of the challenges, solutions, and future directions, this survey aims to facilitate further research and application of on-device AI, ultimately contributing to the advancement of intelligent systems in everyday life.},
    number = {9},
    urldate = {2026-02-10},
    journal = {ACM Comput. Surv.},
    author = {Wang, Xubin and Tang, Zhiqing and Guo, Jianxiong and Meng, Tianhui and Wang, Chenhao and Wang, Tian and Jia, Weijia},
    month = apr,
    year = {2025},
    note = {79 citations (Semantic Scholar/DOI) [2026-02-10]
TLDR: This survey comprehensively explores the current state, technical challenges, and future trends of on-device AI models, defining on-device AI models as those designed to perform local data processing and inference, emphasizing their characteristics such as real-time performance, resource constraints, and enhanced data privacy.},
    pages = {228:1--228:39},
}
@inproceedings{ren_tinyol_2021,
    title = {{TinyOL}: {TinyML} with {Online}-{Learning} on {Microcontrollers}},
    issn = {2161-4407},
    shorttitle = {{TinyOL}},
    url = {https://ieeexplore.ieee.org/abstract/document/9533927},
    doi = {10.1109/IJCNN52387.2021.9533927},
    abstract = {Tiny machine learning (TinyML) is a fast-growing research area committed to democratizing deep learning for all-pervasive microcontrollers (MCUs). Challenged by the constraints on power, memory, and computation, TinyML has achieved significant advancement in the last few years. However, the current TinyML solutions are based on batch/offline setting and support only the neural network's inference on MCUs. The neural network is first trained using a large amount of pre-collected data on a powerful machine and then flashed to MCUs. This results in a static model, hard to adapt to new data, and impossible to adjust for different scenarios, which impedes the flexibility of the Internet of Things (IoT). To address these problems, we propose a novel system called TinyOL (TinyML with Online-Learning), which enables incremental on-device training on streaming data. TinyOL is based on the concept of online learning and is suitable for constrained IoT devices. We experiment TinyOL under supervised and unsupervised setups using an autoencoder neural network. Finally, we report the performance of the proposed solution and show its effectiveness and feasibility.},
    urldate = {2026-02-07},
    booktitle = {2021 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
    author = {Ren, Haoyu and Anicic, Darko and Runkler, Thomas A.},
    month = jul,
    year = {2021},
    note = {169 citations (Semantic Scholar/DOI) [2026-02-07]
ISSN: 2161-4407
TLDR: A novel system called TinyOL (TinyML with Online-Learning), which enables incremental on-device training on streaming data and is suitable for constrained IoT devices is proposed.},
    keywords = {Adaptation models, Deep learning, Memory management, Microcontrollers, Neural networks, Performance evaluation, Training},
    pages = {1--8},
}
@misc{gholami_survey_2021,
    title = {A {Survey} of {Quantization} {Methods} for {Efficient} {Neural} {Network} {Inference}},
    url = {http://arxiv.org/abs/2103.13630},
    doi = {10.48550/arXiv.2103.13630},
    abstract = {As soon as abstract mathematical computations were adapted to computation on digital computers, the problem of efficient representation, manipulation, and communication of the numerical values in those computations arose. Strongly related to the problem of numerical representation is the problem of quantization: in what manner should a set of continuous real-valued numbers be distributed over a fixed discrete set of numbers to minimize the number of bits required and also to maximize the accuracy of the attendant computations? This perennial problem of quantization is particularly relevant whenever memory and/or computational resources are severely restricted, and it has come to the forefront in recent years due to the remarkable performance of Neural Network models in computer vision, natural language processing, and related areas. Moving from floating-point representations to low-precision fixed integer values represented in four bits or less holds the potential to reduce the memory footprint and latency by a factor of 16x; and, in fact, reductions of 4x to 8x are often realized in practice in these applications. Thus, it is not surprising that quantization has emerged recently as an important and very active sub-area of research in the efficient implementation of computations associated with Neural Networks. In this article, we survey approaches to the problem of quantizing the numerical values in deep Neural Network computations, covering the advantages/disadvantages of current methods. With this survey and its organization, we hope to have presented a useful snapshot of the current research in quantization for Neural Networks and to have given an intelligent organization to ease the evaluation of future research in this area.},
    urldate = {2026-02-09},
    publisher = {arXiv},
    author = {Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei and Mahoney, Michael W. and Keutzer, Kurt},
    month = jun,
    year = {2021},
    note = {1393 citations (Semantic Scholar/DOI) [2026-02-09]
arXiv:2103.13630 [cs]},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
}
@article{hubara_quantized_2017,
    title = {Quantized neural networks: training neural networks with low precision weights and activations},
    volume = {18},
    issn = {1532-4435},
    shorttitle = {Quantized neural networks},
    url = {https://dl.acm.org/doi/10.5555/3122009.3242044},
    abstract = {We introduce a method to train Quantized Neural Networks (QNNs) -- neural networks with extremely low precision (e.g., 1-bit) weights and activations, at run-time. At traintime the quantized weights and activations are used for computing the parameter gradients. During the forward pass, QNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations. As a result, power consumption is expected to be drastically reduced. We trained QNNs over the MNIST, CIFAR-10, SVHN and ImageNet datasets. The resulting QNNs achieve prediction accuracy comparable to their 32-bit counterparts. For example, our quantized version of AlexNet with 1-bit weights and 2-bit activations achieves 51\% top-1 accuracy. Moreover, we quantize the parameter gradients to 6-bits as well which enables gradients computation using only bit-wise operation. Quantized recurrent neural networks were tested over the Penn Treebank dataset, and achieved comparable accuracy as their 32-bit counterparts using only 4-bits. Last but not least, we programmed a binary matrix multiplication GPU kernel with which it is possible to run our MNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The QNN code is available online.},
    number = {1},
    urldate = {2026-02-10},
    journal = {J. Mach. Learn. Res.},
    author = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
    month = jan,
    year = {2017},
    pages = {6869--6898},
}
@misc{baalen_fp8_2023,
    title = {{FP8} versus {INT8} for efficient deep learning inference},
    url = {http://arxiv.org/abs/2303.17951},
    doi = {10.48550/arXiv.2303.17951},
    abstract = {Recently, the idea of using FP8 as a number format for neural network training has been floating around the deep learning world. Given that most training is currently conducted with entire networks in FP32, or sometimes FP16 with mixed-precision, the step to having some parts of a network run in FP8 with 8-bit weights is an appealing potential speed-up for the generally costly and time-intensive training procedures in deep learning. A natural question arises regarding what this development means for efficient inference on edge devices. In the efficient inference device world, workloads are frequently executed in INT8. Sometimes going even as low as INT4 when efficiency calls for it. In this whitepaper, we compare the performance for both the FP8 and INT formats for efficient on-device inference. We theoretically show the difference between the INT and FP formats for neural networks and present a plethora of post-training quantization and quantization-aware-training results to show how this theory translates to practice. We also provide a hardware analysis showing that the FP formats are somewhere between 50-180\% less efficient in terms of compute in dedicated hardware than the INT format. Based on our research and a read of the research field, we conclude that although the proposed FP8 format could be good for training, the results for inference do not warrant a dedicated implementation of FP8 in favor of INT8 for efficient inference. We show that our results are mostly consistent with previous findings but that important comparisons between the formats have thus far been lacking. Finally, we discuss what happens when FP8-trained networks are converted to INT8 and conclude with a brief discussion on the most efficient way for on-device deployment and an extensive suite of INT8 results for many models.},
    urldate = {2026-02-09},
    publisher = {arXiv},
    author = {Baalen, Mart van and Kuzmin, Andrey and Nair, Suparna S. and Ren, Yuwei and Mahurin, Eric and Patel, Chirag and Subramanian, Sundar and Lee, Sanghyuk and Nagel, Markus and Soriaga, Joseph and Blankevoort, Tijmen},
    month = jun,
    year = {2023},
    note = {57 citations (Semantic Scholar/arXiv) [2026-02-09]
arXiv:2303.17951 [cs]
Energy Focus
TLDR: This whitepaper compares the performance for both the FP8 and INT formats for efficient on-device inference and theoretically shows the difference between the INT and FP formats for neural networks and presents a plethora of post-training quantization and quantization-aware-training results.},
    keywords = {Computer Science - Machine Learning},
}
@article{tong_enhancing_2026,
    title = {Enhancing {Quantization}-{Aware} {Training} on {Edge} {Devices} {Via} {Relative} {Entropy} {Coreset} {Selection} and {Cascaded} {Layer} {Correction}},
    issn = {1558-0660},
    url = {https://ieeexplore.ieee.org/abstract/document/11341906},
    doi = {10.1109/TMC.2026.3651729},
    abstract = {With the development of mobile and edge computing, the demand for low-bit quantized models on edge devices is increasing to achieve efficient deployment. To enhance the performance, it is often necessary to retrain the quantized models using edge data. However, due to privacy concerns, certain sensitive data can only be processed on edge devices. Therefore, employing Quantization-Aware Training (QAT) on edge devices has become an effective solution. Nevertheless, traditional QAT relies on the complete dataset for training, which incurs a huge computational cost. Coreset selection techniques can mitigate this issue by training on the most representative subsets. However, existing methods struggle to eliminate quantization errors in the model when using small-scale datasets (e.g., only 10\% of the data), leading to significant performance degradation. To address these issues, we propose QuaRC, a QAT framework on edge devices via Relative entropy coreset selection and Cascaded layer correction. The framework consists of two main phases: In the coreset selection phase, QuaRC introduces the “Relative Entropy Score” to identify the subsets that most effectively capture the model's quantization errors. During the training phase, QuaRC employs the Cascaded Layer Correction strategy to align the intermediate layer outputs of the quantized model with those of the full-precision model, thereby effectively reducing the quantization errors in the intermediate layers. Experimental results demonstrate the effectiveness of our approach. For instance, when quantizing ResNet-18 to 2-bit using a 1\% data subset, QuaRC achieves a 5.72\% improvement in Top-1 accuracy on the ImageNet-1 K dataset compared to state-of-the-art techniques.},
    urldate = {2026-02-09},
    journal = {IEEE Transactions on Mobile Computing},
    author = {Tong, Yujia and Yuan, Jingling and Hu, Chuang},
    year = {2026},
    keywords = {Accuracy, Adaptation models, Computational modeling, Data models, Entropy, Noise, Performance evaluation, Quantization (signal), Quantization-Aware Training, Real-time systems, Training, coreset selection, edge computing, efficient training},
    pages = {1--15},
}
@article{hasanpour_survey_2026,
    title = {A {Survey} of {Quantization} {Techniques} in {Embedded} {AI} {Toolchains}: 2025 {IEEE} {Annual} {Congress} on {Artificial} {Intelligence} of {Things}},
    shorttitle = {A {Survey} of {Quantization} {Techniques} in {Embedded} {AI} {Toolchains}},
    abstract = {Quantization has become a key method for enabling deep learning (DL) inference on resource-constrained embedded systems. As the demand for privacy-preserving, low-latency, and energy-efficient artificial intelligence (AI) increases, quantization allows models to run efficiently on edge hardware by reducingthe precision of weights and activations - often with minimal impact on accuracy. This survey presents a tool-centric analysis of quantization support in twelve widely used embedded artificial intelligence (eAI) frameworks, including TensorFlowLite, PyTorch, ONNX Runtime, and vendor-specific stacks like Qualcomm’s QNN and Intel’s OpenVINO. We examine how each tool implements quantization across several axes: supported workflows (post-training vs. quantization-aware training), bit-width flexibility, execution realism (simulated vs. integer kernels),and quantization granularity and schemes. Our findings reveal common patterns — such as the dominance of 8-bit uniform affine quantization—and highlight key distinctions in flexibility, deployment readiness, and hardware integration. We summarize our results in a unified comparison table to guide practitioners and researchers in selecting the most appropriate tool for their deployment needs. Finally, we discuss trends such as mixed precision quantization and speculate on future directions for eAI-tooling},
    journal = {Proceedings of 2025 IEEE Annual Congress on Artificial Intelligence of Things},
    publisher = {IEEE},
    author = {Hasanpour, Amin and Fafoutis, Xenofon and Roveri, Manuel},
    year = {2026},
    keywords = {Deep Learning, Embedded AI, Machine Learning, Quantization, TinyML},
}
@misc{shalavi_energy_2022,
    title = {Energy {Efficient} {Deployment} and {Orchestration} of {Computing} {Resources} at the {Network} {Edge}: a {Survey} on {Algorithms}, {Trends} and {Open} {Challenges}},
    shorttitle = {Energy {Efficient} {Deployment} and {Orchestration} of {Computing} {Resources} at the {Network} {Edge}},
    url = {http://arxiv.org/abs/2209.14141},
    doi = {10.48550/arXiv.2209.14141},
    abstract = {Mobile networks are becoming energy hungry, and this trend is expected to continue due to a surge in communication and computation demand. Multi-access Edge Computing (MEC), will entail energy-consuming services and applications, with non-negligible impact in terms of ecological sustainability. In this paper, we provide a comprehensive review of existing approaches to make edge computing networks greener, including but not limited to the exploitation of renewable energy resources, and context-awareness. We hence provide an updated account of recent developments on MEC from an energetic sustainability perspective, addressing the initial deployment of computing resources, their dynamic (re)allocation, as well as distributed and federated learning designs. In doing so, we highlight the energy aspects of these algorithms, advocating the need for energy-sustainable edge computing systems that are aligned with Sustainable Development Goals (SDGs) and the Paris agreement. To the best of our knowledge, this is the first work providing a systematic literature review on the efficient deployment and management of energy harvesting MEC, with special focus on the deployment, provisioning, and scheduling of computing tasks, including federated learning for distributed edge intelligence, toward making edge networks greener and more sustainable. At the end of the paper, open research avenues and challenges are identified for all the surveyed topics.},
    urldate = {2026-02-07},
    publisher = {arXiv},
    author = {Shalavi, Neda and Perin, Giovanni and Zanella, Andrea and Rossi, Michele},
    month = sep,
    year = {2022},
    note = {8 citations (Semantic Scholar/DOI) [2026-02-07]
arXiv:2209.14141 [cs]
TLDR: This is the first work providing a systematic literature review on the efficient deployment and management of energy harvesting MEC, with special focus on the deployment, provisioning, and scheduling of computing tasks, including federated learning for distributed edge intelligence, toward making edge networks greener and more sustainable.},
    keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Networking and Internet Architecture},
}
@inproceedings{song_simplified_2013,
    title = {A {Simplified} and {Accurate} {Model} of {Power}-{Performance} {Efficiency} on {Emergent} {GPU} {Architectures}},
    issn = {1530-2075},
    url = {https://ieeexplore.ieee.org/abstract/document/6569853},
    doi = {10.1109/IPDPS.2013.73},
    abstract = {Emergent heterogeneous systems must be optimized for both power and performance at exascale. Massive parallelism combined with complex memory hierarchies form a barrier to efficient application and architecture design. These challenges are exacerbated with GPUs as parallelism increases orders of magnitude and power consumption can easily double. Models have been proposed to isolate power and performance bottlenecks and identify their root causes. However, no current models combine simplicity, accuracy, and support for emergent GPU architectures (e.g. NVIDIA Fermi). We combine hardware performance counter data with machine learning and advanced analytics to model power-performance efficiency for modern GPU-based systems. Our performance counter based approach is simpler than previous approaches and does not require detailed understanding of the underlying architecture. The resulting model is accurate for predicting power (within 2.1\%) and performance (within 6.7\%) for application kernels on modern GPUs. Our model can identify power-performance bottlenecks and their root causes for various complex computation and memory access patterns (e.g. global, shared, texture). We measure the accuracy of our power and performance models on a NVIDIA Fermi C2075 GPU for more than a dozen CUDA applications. We show our power model is more accurate and robust than the best available GPU power models - multiple linear regression models MLR and MLR+. We demonstrate how to use our models to identify power-performance bottlenecks and suggest optimization strategies for high-performance codes such as GEM, a biomolecular electrostatic analysis application. We verify our power-performance model is accurate on clusters of NVIDIA Fermi M2090s and useful for suggesting optimal runtime configurations on the Keeneland supercomputer at Georgia Tech.},
    urldate = {2026-02-07},
    booktitle = {2013 {IEEE} 27th {International} {Symposium} on {Parallel} and {Distributed} {Processing}},
    author = {Song, Shuaiwen and Su, Chunyi and Rountree, Barry and Cameron, Kirk W.},
    month = may,
    year = {2013},
    note = {ISSN: 1530-2075},
    keywords = {Adaptation models, Graphics processing units, Kernel, Predictive models, Radiation detectors, Runtime, Training},
    pages = {673--686},
}
@article{fahad_comparative_2019,
    title = {A {Comparative} {Study} of {Methods} for {Measurement} of {Energy} of {Computing}},
    volume = {12},
    copyright = {http://creativecommons.org/licenses/by/3.0/},
    issn = {1996-1073},
    url = {https://www.mdpi.com/1996-1073/12/11/2204},
    doi = {10.3390/en12112204},
    abstract = {Energy of computing is a serious environmental concern and mitigating it is an important technological challenge. Accurate measurement of energy consu...},
    language = {en},
    number = {11},
    urldate = {2026-02-07},
    journal = {Energies},
    publisher = {publisher},
    author = {Fahad, Muhammad and Shahid, Arsalan and Manumachu, Ravi Reddy and Lastovetsky, Alexey},
    month = jun,
    year = {2019},
    note = {TLDR: It is shown that, owing to the nature of the deviations of the energy measurements provided by on-chip sensors from the ground truth, calibration can not improve the accuracy of the on- chip sensors to an extent that can allow them to be used in optimization of applications for dynamic energy.},
    keywords = {GPU, NVML, RAPL, Xeon Phi, energy efficiency, energy predictive models, multicore CPU, performance monitoring counters, power aensors, power meters},
}
@misc{noauthor_nvidia_nodate,
    title = {{NVIDIA} {Management} {Library} ({NVML})},
    url = {https://developer.nvidia.com/management-library-nvml},
    language = {en-US},
    urldate = {2026-02-13},
    journal = {NVIDIA Developer},
}
@inproceedings{aslan_study_2022,
    title = {A {Study} on {Power} and {Energy} {Measurement} of {NVIDIA} {Jetson} {Embedded} {GPUs} {Using} {Built}-in {Sensor}},
    issn = {2521-1641},
    url = {https://ieeexplore.ieee.org/abstract/document/9919522/authors},
    doi = {10.1109/UBMK55850.2022.9919522},
    abstract = {Artificial intelligence (AI) has been shifted to the embedded devices known as edge devices. Component-level power is very important for the design and optimization of applications on edge devices to estimate energy consumption. Thus, accurate power measurements are needed for battery-powered systems. However, it is not straightforward. Because the behavior of a GPU is rather complex and not well documented. In this work, we report challenges getting power measurements using the built-in power sensor for an NVIDIA Jetson GPU device. We provide a method for true power and energy measurements of the kernels running on NVIDIA Jetson family GPUs.},
    urldate = {2026-02-07},
    booktitle = {2022 7th {International} {Conference} on {Computer} {Science} and {Engineering} ({UBMK})},
    author = {Aslan, Büşra and Yilmazer-Metin, Ayse},
    month = sep,
    year = {2022},
    note = {7 citations (Semantic Scholar/DOI) [2026-02-07]
ISSN: 2521-1641},
    keywords = {Artificial intelligence, Behavioral sciences, Computer science, Energy consumption, Energy measurement, Graphics processing units, Power measurement, dynamic power, edge devices, energy calculation, experimentation, internal power sensor, power monitoring, power profiling},
    pages = {1--6},
}
@inproceedings{holly_profiling_2020,
    title = {Profiling {Energy} {Consumption} of {Deep} {Neural} {Networks} on {NVIDIA} {Jetson} {Nano}},
    url = {https://ieeexplore.ieee.org/abstract/document/9290876},
    doi = {10.1109/IGSC51522.2020.9290876},
    abstract = {Improving the capabilities of embedded devices and accelerators for Deep Neural Networks (DNN) leads to a shift from cloud to edge computing. Especially for battery-powered systems, intelligent energy management is critical. In this work, we provide a measurement base for power estimation on NVIDIA Jetson devices. We analyze the effects of different CPU and GPU settings on power consumption, latency, and energy for complete DNNs as well as for individual layers. Furthermore, we provide optimal settings for minimal power and energy consumption for an NVIDIA Jetson Nano.},
    urldate = {2026-02-07},
    booktitle = {2020 11th {International} {Green} and {Sustainable} {Computing} {Workshops} ({IGSC})},
    author = {Holly, Stephan and Wendt, Alexander and Lechner, Martin},
    month = oct,
    year = {2020},
    note = {30 citations (Semantic Scholar/DOI) [2026-02-07]},
    keywords = {Energy consumption, Graphics processing units, Hardware, Jetson, NVIDIA, Neural networks, Power demand, Power measurement, Sensors, deep neural network, energy, latency, power, profiling},
    pages = {1--6},
}
@misc{noauthor_power_2023,
    title = {Power {Optimization} with {NVIDIA} {Jetson}},
    url = {https://developer.nvidia.com/blog/power-optimization-with-nvidia-jetson/},
    abstract = {When working with embedded systems such as the Jetson modules, you must optimize your application based on your power budget and compute resources. To avoid performance or even thermal throttling…},
    language = {en-US},
    urldate = {2026-02-07},
    journal = {NVIDIA Technical Blog},
    month = oct,
    year = {2023},
    keywords = {Domain/Documentation, Sec/Edge, Top/Edge/Orin, ⭐⭐⭐⭐⭐},
}
@article{gill_edge_2024,
    title = {Edge {AI}: {A} {Taxonomy}, {Systematic} {Review} and {Future} {Directions}},
    volume = {28},
    issn = {1573-7543},
    shorttitle = {Edge {AI}},
    url = {https://doi.org/10.1007/s10586-024-04686-y},
    doi = {10.1007/s10586-024-04686-y},
    abstract = {Edge Artificial Intelligence (AI) incorporates a network of interconnected systems and devices that receive, cache, process, and analyse data in close communication with the location where the data is captured with AI technology. Recent advancements in AI efficiency, the widespread use of Internet of Things (IoT) devices, and the emergence of edge computing have unlocked the enormous scope of Edge AI. The goal of Edge AI is to optimize data processing efficiency and velocity while ensuring data confidentiality and integrity. Despite being a relatively new field of research, spanning from 2014 to the present, it has shown significant and rapid development over the last five years. In this article, we present a systematic literature review for Edge AI to discuss the existing research, recent advancements, and future research directions. We created a collaborative edge AI learning system for cloud and edge computing analysis, including an in-depth study of the architectures that facilitate this mechanism. The taxonomy for Edge AI facilitates the classification and configuration of Edge AI systems while also examining its potential influence across many fields through compassing infrastructure, cloud computing, fog computing, services, use cases, ML and deep learning, and resource management. This study highlights the significance of Edge AI in processing real-time data at the edge of the network. Additionally, it emphasizes the research challenges encountered by Edge AI systems, including constraints on resources, vulnerabilities to security threats, and problems with scalability. Finally, this study highlights the potential future research directions that aim to address the current limitations of Edge AI by providing innovative solutions.},
    language = {en},
    number = {1},
    urldate = {2026-02-07},
    journal = {Cluster Computing},
    author = {Gill, Sukhpal Singh and Golec, Muhammed and Hu, Jianmin and Xu, Minxian and Du, Junhui and Wu, Huaming and Walia, Guneet Kaur and Murugesan, Subramaniam Subramanian and Ali, Babar and Kumar, Mohit and Ye, Kejiang and Verma, Prabal and Kumar, Surendra and Cuadrado, Felix and Uhlig, Steve},
    month = oct,
    year = {2024},
    note = {114 citations (Semantic Scholar/DOI) [2026-02-07]
TLDR: This study highlights the significance of Edge AI in processing real-time data at the edge of the network, and emphasizes the research challenges encountered by Edge AI systems, including constraints on resources, vulnerabilities to security threats, and problems with scalability.},
    keywords = {Artificial intelligence, Cloud computing, Edge AI, Edge computing, Machine learning},
    pages = {18},
}
@article{philipo_sustainable_2025,
    title = {Sustainable {AI}: {Emerging} {Trends}, {Impacts}, and {Future} {Challenges}},
    volume = {10},
    issn = {2377-3782},
    shorttitle = {Sustainable {AI}},
    url = {https://ieeexplore.ieee.org/document/11168278},
    doi = {10.1109/TSUSC.2025.3611272},
    abstract = {Sustainable AI considers both the environmental impact of AI technologies and their role in advancing global sustainability. This study provides a comprehensive analysis of emerging trends in sustainable AI, highlighting energy-efficient algorithms, green data centers, AI-driven resource management, and ethical governance. It emphasizes AI’s dual nature, its potential to drive climate action and socio-economic progress, alongside risks such as carbon emissions, e-waste, and digital inequality. The study underscores the need for interdisciplinary collaboration, inclusive policies, and standardized sustainability metrics to ensure responsible AI deployment. By identifying barriers and proposing strategies, it offers guidance to researchers, policymakers, and industry leaders on aligning AI with long-term environmental, ethical, and social objectives.},
    number = {6},
    urldate = {2026-02-10},
    journal = {IEEE Transactions on Sustainable Computing},
    author = {Philipo, Adamu Gaston and Ning, Huansheng and Sarwatt, Doreen Sebastian and Mohamed, Jumanne Ally and Yusufu, Afidhu Swaibu and Shi, Feifei and Urenje, Shepherd and Ding, Jianguo},
    month = nov,
    year = {2025},
    note = {0 citations (Semantic Scholar/DOI) [2026-02-10]},
    keywords = {AI governance, Artificial intelligence, Cooling, Data mining, Electronic waste, Energy consumption, Energy efficiency, Ethics, Green products, Measurement, Sustainable AI, Sustainable development, energy efficiency, environmental impact, ethical AI, green AI},
    pages = {1278--1291},
}
@misc{noauthor_jetson-stats_nodate,
    title = {jetson-stats},
    url = {https://rnext.it/jetson_stats/index.html},
    abstract = {jetson-stats is a package for monitoring and control your NVIDIA Jetson [Thor, Orin, Xavier, Nano, TX] series. Works with all NVIDIA Jetson ecosystem. Installing: jetson-stats can be installed with...},
    language = {en},
    urldate = {2026-02-07},
    journal = {jetson-stats},
    keywords = {⭐⭐⭐⭐},
}
@article{surianarayanan_survey_2023,
    title = {A {Survey} on {Optimization} {Techniques} for {Edge} {Artificial} {Intelligence} ({AI})},
    volume = {23},
    copyright = {http://creativecommons.org/licenses/by/3.0/},
    issn = {1424-8220},
    url = {https://www.mdpi.com/1424-8220/23/3/1279},
    doi = {10.3390/s23031279},
    abstract = {Artificial Intelligence (Al) models are being produced and used to solve a variety of current and future business and technical problems. Therefore, A...},
    language = {en},
    number = {3},
    urldate = {2026-02-07},
    journal = {Sensors},
    publisher = {publisher},
    author = {Surianarayanan, Chellammal and Lawrence, John Jeyasekaran and Chelliah, Pethuru Raj and Prakash, Edmond and Hewage, Chaminda},
    month = jan,
    year = {2023},
    note = {82 citations (Semantic Scholar/DOI) [2026-02-07]
TLDR: This paper is to dig deep and describe all kinds of model optimization at different levels and layers and highlighted the importance of having an enabling AI model optimization framework.},
    keywords = {AI model optimization, artificial intelligence, edge AI, energy efficient methods for edge AI, federated learning, optimization methods for edge AI},
}
@article{salehi_data-centric_2024,
    title = {Data-{Centric} {Green} {Artificial} {Intelligence}: {A} {Survey}},
    volume = {5},
    issn = {2691-4581},
    shorttitle = {Data-{Centric} {Green} {Artificial} {Intelligence}},
    url = {https://ieeexplore.ieee.org/document/10251541},
    doi = {10.1109/TAI.2023.3315272},
    abstract = {With the exponential growth of computational power and the availability of large-scale datasets in recent years, remarkable advancements have been made in the field of artificial intelligence (AI), leading to complex models and innovative applications. However, these models consume a significant unprecedented amount of energy, contributing to greenhouse gas emissions and a growing carbon footprint in the AI industry. In response, the concept of green AI has emerged, prioritizing energy efficiency and sustainability alongside accuracy and related measures. To this end, data-centric approaches are very promising to reduce the energy consumption of AI algorithms. This article presents a comprehensive overview of data-centric technologies and their impact on the energy efficiency of AI algorithms. Specifically, it focuses on methods that utilize training data in an efficient manner to improve the energy efficiency of AI algorithms. We have identified multiple data-centric approaches, such as active learning, knowledge transfer/sharing, dataset distillation, data augmentation, and curriculum learning that can contribute to the development of environmentally-friendly implementations of machine learning algorithms. Finally, the practical applications of these approaches are highlighted, and the challenges and future directions in the field are discussed.},
    number = {5},
    urldate = {2026-02-10},
    journal = {IEEE Transactions on Artificial Intelligence},
    author = {Salehi, Shirin and Schmeink, Anke},
    month = may,
    year = {2024},
    note = {44 citations (Semantic Scholar/DOI) [2026-02-10]},
    keywords = {Artificial intelligence, Computational efficiency, Computational modeling, Data-centric artificial intelligence (AI) (DCAI), Energy efficiency, Green products, Machine learning, Training, data-efficiency, energy-efficiency, green AI},
    pages = {1973--1989},
}
@article{kumar_towards_2025,
    title = {Towards {Green} {AI}: {A} {Novel} {Hybrid} {Filter} {Based} {AI} {Approach} for {Energy} {Efficient} {State} of {Charge} and {Energy} {Estimation} in {Li}-{Ion} {Batteries} {Under} {Various} {Drive} {Cycles}},
    volume = {61},
    issn = {1939-9367},
    shorttitle = {Towards {Green} {AI}},
    url = {https://ieeexplore.ieee.org/document/10922161},
    doi = {10.1109/TIA.2025.3550108},
    abstract = {Artificial intelligence models usually need a hefty dataset, which indeed requires more computational time, energy consumption, and laborious work to train the models to achieve accurate results. These models consume more energy to deliver output. Therefore, in this work proposes a novel filter technique (FT) based on an average reduction method. This approach improves the quality of the dataset, i.e., reduces volume and averages similar data to find the precision. This novel FT was incorporated with the convolutional neural network with bidirectional long short-term memory network (FT-CNN-Bi-LSTM) model and tested on four dynamic drive cycle circumstances. The key objective of this modelling technique is to enhance the performance and computational efficiency of the state of charge (SOC) and state of energy (SOE) of rechargeable battery systems such as lithium-ion batteries. A comparison examination was performed by proposed models configured with FT and traditional models, including FT-CNN, FT-LSTM, FT-BiLSTM, and FT-CNN-LSTM. The comparison examines SOC and SOE estimation efficacy and the model's capabilities and limitations. The FT-CNN-BiLSTM model was significantly tested on multiple dynamic drive cycles, including HWFET, LA92, UDDS, and US06. These drive cycle datasets were collected from our laboratory, where experiments were conducted at ambient temperature. The FT-CNN-Bi-LSTM approach achieved percentage improvements, 99.98\% of RMSE, 99.97\% of MSE, and 99.96\% of MAE. The significant percentage improvement in computational time and energy consumption ranges from 59 to 66\% and 61 to 68\%. These results demonstrate superior efficiency for rapid SOC and SOE estimation in all dynamic drive cycles.},
    number = {5},
    urldate = {2026-02-10},
    journal = {IEEE Transactions on Industry Applications},
    author = {Kumar, Deepak and Rizwan, M. and Panwar, Amrish K.},
    month = sep,
    year = {2025},
    note = {6 citations (Semantic Scholar/DOI) [2026-02-10]},
    keywords = {Accuracy, Artificial intelligence, Batteries, Computational modeling, Data models, Estimation, Filtering, Mathematical models, State of charge, Training, drive cycles, electric vehicles, lithium-ion batteries, state of charge (SOC) and energy estimation},
    pages = {7633--7645},
}
@article{kumar_towards_2026,
    title = {Towards {Green} {AI}: {Energy}-{Efficient} {State} of {Health} {Estimation} and {Degradation} {Analysis} of {Commercial} {Lithium}-{Ion} {Batteries} {Based} on {Deep} {Learning} and {Filter} {Technique} {Approach}},
    volume = {62},
    issn = {1939-9367},
    shorttitle = {Towards {Green} {AI}},
    url = {https://ieeexplore.ieee.org/document/11146426},
    doi = {10.1109/TIA.2025.3604739},
    abstract = {The redundant similar data points in the large datasets increase the dataset volume, storage, memory usage, training time, and computational resources, leading to significant inefficiency in the deep learning (DL) models. These inefficiencies reduce model performance and increase energy consumption. The existing DL-based approaches for state of health (SOH) estimation of lithium-ion batteries often face challenges such as high computational demands, low accuracy, and high energy consumption. These models require high energy for accurate results and contribute to higher electricity demand and carbon footprints. Therefore, this work proposes a novel filter technique (FT) based on the redundancy reduction approach. This approach improves the quality of the dataset, i.e., reduces dataset size, memory utilization, and energy consumption. This novel FT was incorporated with the gated recurrent unit (FT-GRU) model and tested on six different types of SOH datasets. The key objective of this approach is to enhance performance and lower energy consumption. A comparison examination was performed by proposed models configured with FT and traditional models to investigate the SOH estimation effectiveness and the model’s abilities and limits. The FT-GRU model was significantly tested on laboratory and publicly available NASA dynamic datasets, including B1, B2, B05, B06, B07 and B18. The B1 and B2 datasets were collected from our laboratory at room temperature. The FT-GRU approach achieved percentage improvements of 75.96\% of RMSE, 99.49\% of MSE, 93.07\% of MAE, and computational time and energy consumption ranges from 25\% to 70.76\% and 69.50\% to 82.38\%, respectively, in comparison to the other existing approaches.},
    number = {2},
    urldate = {2026-02-10},
    journal = {IEEE Transactions on Industry Applications},
    author = {Kumar, Deepak and Ahmed, Mujeeb and Jamil, Majid and Rizwan, M. and Panwar, Amrish K.},
    month = mar,
    year = {2026},
    note = {0 citations (Semantic Scholar/DOI) [2026-02-10]},
    keywords = {Accuracy, Adaptation models, Artificial intelligence, Computational efficiency, Computational modeling, Degradation, Energy consumption, Energy efficiency, Estimation, Green AI, Training, degradation analysis, energy efficiency DL models, lithium-ion batteries, state of health},
    pages = {3009--3022},
}
@article{kim_eco-friendly_2023,
    title = {Eco-{Friendly} {Low} {Resource} {Security} {Surveillance} {Framework} {Toward} {Green} {AI} {Digital} {Twin}},
    volume = {27},
    issn = {1558-2558},
    url = {https://ieeexplore.ieee.org/document/9932580},
    doi = {10.1109/LCOMM.2022.3218050},
    abstract = {Most intelligent systems focused on how to improve performance including accuracy, processing speed with a massive number of data sets and those performance-biased intelligent systems, Red AI systems, have been applied to digital twin in smart cities. On the other hand, it is highly reasonable to consider Green AI features covering environmental, economic, social costs for advanced digital twin services. In this letter, we propose eco-friendly low resource security surveillance toward Green AI-enabled digital twin service, which provides eco-friendly security by the active participation of low resource devices. And, we formally define a problem whose objective is to maximize the participation of low source or reusable devices such that reusable surveillance borders are created within security district. Also, a dense sub-district with low resource devices priority completion scheme is proposed to resolve the problem. Then, the devised method is performed by expanded simulations and the achieved result is evaluated with demonstrated discussions.},
    number = {1},
    urldate = {2026-02-10},
    journal = {IEEE Communications Letters},
    author = {Kim, Hyunbum and Ben-Othman, Jalel},
    month = jan,
    year = {2023},
    note = {31 citations (Semantic Scholar/DOI) [2026-02-10]},
    keywords = {Artificial intelligence, Digital twins, Eco-friendly, Green products, Object recognition, Performance evaluation, Security, Surveillance, digital twin, low resource, security, surveillance},
    pages = {377--380},
}