@manual{nvidia_orin_nano_datasheet,
    title        = {NVIDIA Jetson Orin Nano Series Data Sheet},
    author       = {{NVIDIA Corporation}},
    number       = {DS-11105-001\_v1.1},
    year         = {2023},
    organization = {NVIDIA},
    note         = {Accessed: 2026-02-13}
}
@misc{birkl_oxford_2017,
  title        = {Oxford Battery Degradation Dataset 1},
  author       = {Birkl, Christoph R.},
  year         = {2017},
  howpublished = {\url{https://doi.org/10.5287/bodleian:KZ8DMw58g}},
  note         = {University of Oxford},
}

@misc{calce_battery_data,
  title        = {Battery Data},
  author       = {{Center for Advanced Life Cycle Engineering}},
  year         = {2012},
  howpublished = {\url{https://calce.umd.edu/battery-data}},
  note         = {University of Maryland},
}

@article{wang_pinn4soh_2024,
  title   = {Physics-informed neural network for lithium-ion battery degradation},
  author  = {Wang, Fujin and others},
  journal = {Nature Communications},
  year    = {2024},
  volume  = {15},
  number  = {1},
  pages   = {xxxx--xxxx},
  doi     = {10.1038/s41467-024-48779-z},
}

@article{rosendo_distributed_2022,
    title = {Distributed intelligence on the {Edge}-to-{Cloud} {Continuum}: {A} systematic literature review},
    volume = {166},
    issn = {0743-7315},
    shorttitle = {Distributed intelligence on the {Edge}-to-{Cloud} {Continuum}},
    url = {https://www.sciencedirect.com/science/article/pii/S0743731522000843},
    doi = {10.1016/j.jpdc.2022.04.004},
    abstract = {The explosion of data volumes generated by an increasing number of applications is strongly impacting the evolution of distributed digital infrastructures for data analytics and machine learning (ML). While data analytics used to be mainly performed on cloud infrastructures, the rapid development of IoT infrastructures and the requirements for low-latency, secure processing has motivated the development of edge analytics. Today, to balance various trade-offs, ML-based analytics tends to increasingly leverage an interconnected ecosystem that allows complex applications to be executed on hybrid infrastructures where IoT Edge devices are interconnected to Cloud/HPC systems in what is called the Computing Continuum, the Digital Continuum, or the Transcontinuum. Enabling learning-based analytics on such complex infrastructures is challenging. The large scale and optimized deployment of learning-based workflows across the Edge-to-Cloud Continuum requires extensive and reproducible experimental analysis of the application execution on representative testbeds. This is necessary to help understand the performance trade-offs that result from combining a variety of learning paradigms and supportive frameworks. A thorough experimental analysis requires the assessment of the impact of multiple factors, such as: model accuracy, training time, network overhead, energy consumption, processing latency, among others. This review aims at providing a comprehensive vision of the main state-of-the-art libraries and frameworks for machine learning and data analytics available today. It describes the main learning paradigms enabling learning-based analytics on the Edge-to-Cloud Continuum. The main simulation, emulation, deployment systems, and testbeds for experimental research on the Edge-to-Cloud Continuum available today are also surveyed. Furthermore, we analyze how the selected systems provide support for experiment reproducibility. We conclude our review with a detailed discussion of relevant open research challenges and of future directions in this domain such as: holistic understanding of performance; performance optimization of applications; efficient deployment of Artificial Intelligence (AI) workflows on highly heterogeneous infrastructures; and reproducible analysis of experiments on the Computing Continuum.},
    urldate = {2026-02-07},
    journal = {Journal of Parallel and Distributed Computing},
    author = {Rosendo, Daniel and Costan, Alexandru and Valduriez, Patrick and Antoniu, Gabriel},
    month = aug,
    year = {2022},
    note = {109 citations (Semantic Scholar/DOI) [2026-02-07]
TLDR: This review aims at providing a comprehensive vision of the main state-of-the-art libraries and frameworks for machine learning and data analytics available today and describes the main learning paradigms enabling learning-based analytics on the Edge-to-Cloud Continuum.},
    keywords = {Big Data Analytics, Computing Continuum, Distributed intelligence, Edge computing, Reproducibility},
    pages = {71--94},
}
@article{mao_green_2024,
    title = {Green {Edge} {AI}: {A} {Contemporary} {Survey}},
    volume = {112},
    issn = {1558-2256},
    shorttitle = {Green {Edge} {AI}},
    url = {https://ieeexplore.ieee.org/document/10637271},
    doi = {10.1109/JPROC.2024.3437365},
    abstract = {Artificial intelligence (AI) technologies have emerged as pivotal enablers across a multitude of industries, including consumer electronics, healthcare, and manufacturing, largely due to their significant resurgence over the past decade. The transformative power of AI is primarily derived from the utilization of deep neural networks (DNNs), which require extensive data for training and substantial computational resources for processing. Consequently, DNN models are typically trained and deployed on resource-rich cloud servers. However, due to potential latency issues associated with cloud communications, deep learning (DL) workflows (e.g., DNN training and inference) are increasingly being transitioned to wireless edge networks in proximity to end-user devices (EUDs). This shift is designed to support latency-sensitive applications and has given rise to a new paradigm of edge AI, which will play a critical role in upcoming sixth-generation (6G) networks to support ubiquitous AI applications. Despite its considerable potential, edge AI faces substantial challenges, mostly due to the dichotomy between the resource limitations of wireless edge networks and the resource-intensive nature of DL. Specifically, the acquisition of large-scale data, as well as the training and inference processes of DNNs, can rapidly deplete the battery energy of EUDs. This necessitates an energy-conscious approach to edge AI to ensure both optimal and sustainable performance. In this article, we present a contemporary survey on green edge AI. We commence by analyzing the principal energy consumption components of edge AI systems to identify the fundamental design principles of green edge AI. Guided by these principles, we then explore energy-efficient design methodologies for the three critical tasks in edge AI systems, including training data acquisition, edge training, and edge inference. Finally, we underscore potential future research directions to further enhance the energy efficiency (EE) of edge AI.},
    number = {7},
    urldate = {2026-02-10},
    journal = {Proceedings of the IEEE},
    author = {Mao, Yuyi and Yu, Xianghao and Huang, Kaibin and Angela Zhang, Ying-Jun and Zhang, Jun},
    month = jul,
    year = {2024},
    note = {62 citations (Semantic Scholar/DOI) [2026-02-10]
TLDR: A contemporary survey on green edge AI is presented, analyzing the principal energy consumption components of edge AI systems to identify the fundamental design principles of green edge AI, and exploring energy-efficient design methodologies for the three critical tasks in edge AI systems, including training data acquisition, edge training, and edge inference.},
    keywords = {6G mobile communication, Artificial intelligence, Cloud computing, Data acquisition, Edge AI, Edge computing, Energy consumption, Energy efficiency, Federated learning, Surveys, Training, Wireless networks, edge artificial intelligence (AI), edge inference, energy efficiency (EE), federated learning (FL), green AI, mobile edge computing (MEC), sixth-generation (6G) wireless networks},
    pages = {880--911},
}
@article{barbierato_toward_2024,
    title = {Toward {Green} {AI}: {A} {Methodological} {Survey} of the {Scientific} {Literature}},
    volume = {12},
    issn = {2169-3536},
    shorttitle = {Toward {Green} {AI}},
    url = {https://ieeexplore.ieee.org/document/10418137},
    doi = {10.1109/ACCESS.2024.3360705},
    abstract = {The pervasive deployment of Deep Learning models has recently prompted apprehensions regarding their ecological footprint, owing to the exorbitant levels of energy consumption necessitated by the training and inference processes. The term “Red AI” is employed to denote artificial intelligence (AI) models that undergo training using resource-intensive methodologies on very large datasets. This practice can engender substantial energy usage and emissions of carbon, thereby opposing “Green AI. ” The latter concept alludes to AI models designed for similar efficiency and reduced environmental impact. This objective is realized through the utilization of smaller datasets, less computationally intensive training techniques, or sustainable energy resources. While Red AI prioritizes accuracy and performance, Green AI emphasizes efficiency and sustainability. Given that both paradigms exhibit advantages and limitations, the debates around the topics have burgeoned in the scientific arena, delving into novel algorithms, hardware innovations, and improved data utilization techniques aimed at mitigating the ecological consequences of intricate applications such as GPT and BERT. Nevertheless, due to the relative novelty of this debate, not much effort has been dedicated yet to contextualizing the essence of Red AI and the prospects of Green AI in a coherent framework. Within this context, the present work contributes by meticulously delineating both domains through a multifaceted analysis of their causes and ramifications, described from the points of computer architectures, data structures, and algorithms. Additionally, the study reviews notable instances of study cases based on complex Red AI models. The primary contribution of this article encompasses a comprehensive survey of Red and Green AI, stemming from a selection of the literature performed by the authors, subsequently organized into distinct clusters. These clusters encompass i) articles that qualitatively or quantitatively address the issue of Red AI, identifying Green AI as a plausible remedy, ii) articles offering insights into the environmental impact associated with the deployment of extensive Deep Learning models, and iii) articles introducing the techniques underpinning Green AI, aiming at mitigating the cost of Red AI. The outcome emerging from the analysis performed by this work consists of a compromise between sustainability in contrast to the performance of AI tools. Unless the complex training and inference procedures of software models mitigate their environmental impact, it will be necessary to decrease the level of accuracy of production systems, inevitably conflicting with the objective of the major AI vendors. The outcomes of this work would be beneficial to scholars pursuing intricate Deep Learning architectures in scientific research, as well as AI enterprises struggling with the protracted training demands of commercial products within the realms of Computer Vision and Natural Language Processing.},
    urldate = {2026-02-10},
    journal = {IEEE Access},
    author = {Barbierato, Enrico and Gatti, Alice},
    year = {2024},
    note = {30 citations (Semantic Scholar/DOI) [2026-02-10]
TLDR: A comprehensive survey of Red and Green AI is undertaken, meticulously delineating both domains through a multifaceted analysis of their causes and ramifications, described from the points of computer architectures, data structures, and algorithms.},
    keywords = {Artificial intelligence, Biological system modeling, Computational modeling, Computer architecture, Environmental monitoring, Green AI, Green products, Surveys, Training, environmental impact, red AI, survey},
    pages = {23989--24013},
}
@article{wang_empowering_2025,
    title = {Empowering {Edge} {Intelligence}: {A} {Comprehensive} {Survey} on {On}-{Device} {AI} {Models}},
    volume = {57},
    issn = {0360-0300},
    shorttitle = {Empowering {Edge} {Intelligence}},
    url = {https://dl.acm.org/doi/10.1145/3724420},
    doi = {10.1145/3724420},
    abstract = {The rapid advancement of artificial intelligence (AI) technologies has led to an increasing deployment of AI models on edge and terminal devices, driven by the proliferation of the Internet of Things (IoT) and the need for real-time data processing. This survey comprehensively explores the current state, technical challenges, and future trends of on-device AI models. We define on-device AI models as those designed to perform local data processing and inference, emphasizing their characteristics such as real-time performance, resource constraints, and enhanced data privacy. The survey is structured around key themes, including the fundamental concepts of AI models, application scenarios across various domains, and technical challenges faced in edge environments. We also discuss optimization and implementation strategies, such as data preprocessing, model compression, and hardware acceleration, which are essential for effective deployment. Furthermore, we examine the impact of emerging technologies, including edge computing and foundation models, on the evolution of on-device AI models. By providing a structured overview of the challenges, solutions, and future directions, this survey aims to facilitate further research and application of on-device AI, ultimately contributing to the advancement of intelligent systems in everyday life.},
    number = {9},
    urldate = {2026-02-10},
    journal = {ACM Comput. Surv.},
    author = {Wang, Xubin and Tang, Zhiqing and Guo, Jianxiong and Meng, Tianhui and Wang, Chenhao and Wang, Tian and Jia, Weijia},
    month = apr,
    year = {2025},
    note = {79 citations (Semantic Scholar/DOI) [2026-02-10]
TLDR: This survey comprehensively explores the current state, technical challenges, and future trends of on-device AI models, defining on-device AI models as those designed to perform local data processing and inference, emphasizing their characteristics such as real-time performance, resource constraints, and enhanced data privacy.},
    pages = {228:1--228:39},
}
@inproceedings{ren_tinyol_2021,
    title = {{TinyOL}: {TinyML} with {Online}-{Learning} on {Microcontrollers}},
    issn = {2161-4407},
    shorttitle = {{TinyOL}},
    url = {https://ieeexplore.ieee.org/abstract/document/9533927},
    doi = {10.1109/IJCNN52387.2021.9533927},
    abstract = {Tiny machine learning (TinyML) is a fast-growing research area committed to democratizing deep learning for all-pervasive microcontrollers (MCUs). Challenged by the constraints on power, memory, and computation, TinyML has achieved significant advancement in the last few years. However, the current TinyML solutions are based on batch/offline setting and support only the neural network's inference on MCUs. The neural network is first trained using a large amount of pre-collected data on a powerful machine and then flashed to MCUs. This results in a static model, hard to adapt to new data, and impossible to adjust for different scenarios, which impedes the flexibility of the Internet of Things (IoT). To address these problems, we propose a novel system called TinyOL (TinyML with Online-Learning), which enables incremental on-device training on streaming data. TinyOL is based on the concept of online learning and is suitable for constrained IoT devices. We experiment TinyOL under supervised and unsupervised setups using an autoencoder neural network. Finally, we report the performance of the proposed solution and show its effectiveness and feasibility.},
    urldate = {2026-02-07},
    booktitle = {2021 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
    author = {Ren, Haoyu and Anicic, Darko and Runkler, Thomas A.},
    month = jul,
    year = {2021},
    note = {169 citations (Semantic Scholar/DOI) [2026-02-07]
ISSN: 2161-4407
TLDR: A novel system called TinyOL (TinyML with Online-Learning), which enables incremental on-device training on streaming data and is suitable for constrained IoT devices is proposed.},
    keywords = {Adaptation models, Deep learning, Memory management, Microcontrollers, Neural networks, Performance evaluation, Training},
    pages = {1--8},
}
@misc{gholami_survey_2021,
    title = {A {Survey} of {Quantization} {Methods} for {Efficient} {Neural} {Network} {Inference}},
    url = {http://arxiv.org/abs/2103.13630},
    doi = {10.48550/arXiv.2103.13630},
    abstract = {As soon as abstract mathematical computations were adapted to computation on digital computers, the problem of efficient representation, manipulation, and communication of the numerical values in those computations arose. Strongly related to the problem of numerical representation is the problem of quantization: in what manner should a set of continuous real-valued numbers be distributed over a fixed discrete set of numbers to minimize the number of bits required and also to maximize the accuracy of the attendant computations? This perennial problem of quantization is particularly relevant whenever memory and/or computational resources are severely restricted, and it has come to the forefront in recent years due to the remarkable performance of Neural Network models in computer vision, natural language processing, and related areas. Moving from floating-point representations to low-precision fixed integer values represented in four bits or less holds the potential to reduce the memory footprint and latency by a factor of 16x; and, in fact, reductions of 4x to 8x are often realized in practice in these applications. Thus, it is not surprising that quantization has emerged recently as an important and very active sub-area of research in the efficient implementation of computations associated with Neural Networks. In this article, we survey approaches to the problem of quantizing the numerical values in deep Neural Network computations, covering the advantages/disadvantages of current methods. With this survey and its organization, we hope to have presented a useful snapshot of the current research in quantization for Neural Networks and to have given an intelligent organization to ease the evaluation of future research in this area.},
    urldate = {2026-02-09},
    publisher = {arXiv},
    author = {Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei and Mahoney, Michael W. and Keutzer, Kurt},
    month = jun,
    year = {2021},
    note = {1393 citations (Semantic Scholar/DOI) [2026-02-09]
arXiv:2103.13630 [cs]},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
}
@article{hubara_quantized_2017,
    title = {Quantized neural networks: training neural networks with low precision weights and activations},
    volume = {18},
    issn = {1532-4435},
    shorttitle = {Quantized neural networks},
    url = {https://dl.acm.org/doi/10.5555/3122009.3242044},
    abstract = {We introduce a method to train Quantized Neural Networks (QNNs) -- neural networks with extremely low precision (e.g., 1-bit) weights and activations, at run-time. At traintime the quantized weights and activations are used for computing the parameter gradients. During the forward pass, QNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations. As a result, power consumption is expected to be drastically reduced. We trained QNNs over the MNIST, CIFAR-10, SVHN and ImageNet datasets. The resulting QNNs achieve prediction accuracy comparable to their 32-bit counterparts. For example, our quantized version of AlexNet with 1-bit weights and 2-bit activations achieves 51\% top-1 accuracy. Moreover, we quantize the parameter gradients to 6-bits as well which enables gradients computation using only bit-wise operation. Quantized recurrent neural networks were tested over the Penn Treebank dataset, and achieved comparable accuracy as their 32-bit counterparts using only 4-bits. Last but not least, we programmed a binary matrix multiplication GPU kernel with which it is possible to run our MNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The QNN code is available online.},
    number = {1},
    urldate = {2026-02-10},
    journal = {J. Mach. Learn. Res.},
    author = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
    month = jan,
    year = {2017},
    pages = {6869--6898},
}
@misc{baalen_fp8_2023,
    title = {{FP8} versus {INT8} for efficient deep learning inference},
    url = {http://arxiv.org/abs/2303.17951},
    doi = {10.48550/arXiv.2303.17951},
    abstract = {Recently, the idea of using FP8 as a number format for neural network training has been floating around the deep learning world. Given that most training is currently conducted with entire networks in FP32, or sometimes FP16 with mixed-precision, the step to having some parts of a network run in FP8 with 8-bit weights is an appealing potential speed-up for the generally costly and time-intensive training procedures in deep learning. A natural question arises regarding what this development means for efficient inference on edge devices. In the efficient inference device world, workloads are frequently executed in INT8. Sometimes going even as low as INT4 when efficiency calls for it. In this whitepaper, we compare the performance for both the FP8 and INT formats for efficient on-device inference. We theoretically show the difference between the INT and FP formats for neural networks and present a plethora of post-training quantization and quantization-aware-training results to show how this theory translates to practice. We also provide a hardware analysis showing that the FP formats are somewhere between 50-180\% less efficient in terms of compute in dedicated hardware than the INT format. Based on our research and a read of the research field, we conclude that although the proposed FP8 format could be good for training, the results for inference do not warrant a dedicated implementation of FP8 in favor of INT8 for efficient inference. We show that our results are mostly consistent with previous findings but that important comparisons between the formats have thus far been lacking. Finally, we discuss what happens when FP8-trained networks are converted to INT8 and conclude with a brief discussion on the most efficient way for on-device deployment and an extensive suite of INT8 results for many models.},
    urldate = {2026-02-09},
    publisher = {arXiv},
    author = {Baalen, Mart van and Kuzmin, Andrey and Nair, Suparna S. and Ren, Yuwei and Mahurin, Eric and Patel, Chirag and Subramanian, Sundar and Lee, Sanghyuk and Nagel, Markus and Soriaga, Joseph and Blankevoort, Tijmen},
    month = jun,
    year = {2023},
    note = {57 citations (Semantic Scholar/arXiv) [2026-02-09]
arXiv:2303.17951 [cs]
Energy Focus
TLDR: This whitepaper compares the performance for both the FP8 and INT formats for efficient on-device inference and theoretically shows the difference between the INT and FP formats for neural networks and presents a plethora of post-training quantization and quantization-aware-training results.},
    keywords = {Computer Science - Machine Learning},
}
@article{tong_enhancing_2026,
    title = {Enhancing {Quantization}-{Aware} {Training} on {Edge} {Devices} {Via} {Relative} {Entropy} {Coreset} {Selection} and {Cascaded} {Layer} {Correction}},
    issn = {1558-0660},
    url = {https://ieeexplore.ieee.org/abstract/document/11341906},
    doi = {10.1109/TMC.2026.3651729},
    abstract = {With the development of mobile and edge computing, the demand for low-bit quantized models on edge devices is increasing to achieve efficient deployment. To enhance the performance, it is often necessary to retrain the quantized models using edge data. However, due to privacy concerns, certain sensitive data can only be processed on edge devices. Therefore, employing Quantization-Aware Training (QAT) on edge devices has become an effective solution. Nevertheless, traditional QAT relies on the complete dataset for training, which incurs a huge computational cost. Coreset selection techniques can mitigate this issue by training on the most representative subsets. However, existing methods struggle to eliminate quantization errors in the model when using small-scale datasets (e.g., only 10\% of the data), leading to significant performance degradation. To address these issues, we propose QuaRC, a QAT framework on edge devices via Relative entropy coreset selection and Cascaded layer correction. The framework consists of two main phases: In the coreset selection phase, QuaRC introduces the “Relative Entropy Score” to identify the subsets that most effectively capture the model's quantization errors. During the training phase, QuaRC employs the Cascaded Layer Correction strategy to align the intermediate layer outputs of the quantized model with those of the full-precision model, thereby effectively reducing the quantization errors in the intermediate layers. Experimental results demonstrate the effectiveness of our approach. For instance, when quantizing ResNet-18 to 2-bit using a 1\% data subset, QuaRC achieves a 5.72\% improvement in Top-1 accuracy on the ImageNet-1 K dataset compared to state-of-the-art techniques.},
    urldate = {2026-02-09},
    journal = {IEEE Transactions on Mobile Computing},
    author = {Tong, Yujia and Yuan, Jingling and Hu, Chuang},
    year = {2026},
    keywords = {Accuracy, Adaptation models, Computational modeling, Data models, Entropy, Noise, Performance evaluation, Quantization (signal), Quantization-Aware Training, Real-time systems, Training, coreset selection, edge computing, efficient training},
    pages = {1--15},
}
@article{hasanpour_survey_2026,
    title = {A {Survey} of {Quantization} {Techniques} in {Embedded} {AI} {Toolchains}: 2025 {IEEE} {Annual} {Congress} on {Artificial} {Intelligence} of {Things}},
    shorttitle = {A {Survey} of {Quantization} {Techniques} in {Embedded} {AI} {Toolchains}},
    abstract = {Quantization has become a key method for enabling deep learning (DL) inference on resource-constrained embedded systems. As the demand for privacy-preserving, low-latency, and energy-efficient artificial intelligence (AI) increases, quantization allows models to run efficiently on edge hardware by reducingthe precision of weights and activations - often with minimal impact on accuracy. This survey presents a tool-centric analysis of quantization support in twelve widely used embedded artificial intelligence (eAI) frameworks, including TensorFlowLite, PyTorch, ONNX Runtime, and vendor-specific stacks like Qualcomm’s QNN and Intel’s OpenVINO. We examine how each tool implements quantization across several axes: supported workflows (post-training vs. quantization-aware training), bit-width flexibility, execution realism (simulated vs. integer kernels),and quantization granularity and schemes. Our findings reveal common patterns — such as the dominance of 8-bit uniform affine quantization—and highlight key distinctions in flexibility, deployment readiness, and hardware integration. We summarize our results in a unified comparison table to guide practitioners and researchers in selecting the most appropriate tool for their deployment needs. Finally, we discuss trends such as mixed precision quantization and speculate on future directions for eAI-tooling},
    journal = {Proceedings of 2025 IEEE Annual Congress on Artificial Intelligence of Things},
    publisher = {IEEE},
    author = {Hasanpour, Amin and Fafoutis, Xenofon and Roveri, Manuel},
    year = {2026},
    keywords = {Deep Learning, Embedded AI, Machine Learning, Quantization, TinyML},
}
@misc{shalavi_energy_2022,
    title = {Energy {Efficient} {Deployment} and {Orchestration} of {Computing} {Resources} at the {Network} {Edge}: a {Survey} on {Algorithms}, {Trends} and {Open} {Challenges}},
    shorttitle = {Energy {Efficient} {Deployment} and {Orchestration} of {Computing} {Resources} at the {Network} {Edge}},
    url = {http://arxiv.org/abs/2209.14141},
    doi = {10.48550/arXiv.2209.14141},
    abstract = {Mobile networks are becoming energy hungry, and this trend is expected to continue due to a surge in communication and computation demand. Multi-access Edge Computing (MEC), will entail energy-consuming services and applications, with non-negligible impact in terms of ecological sustainability. In this paper, we provide a comprehensive review of existing approaches to make edge computing networks greener, including but not limited to the exploitation of renewable energy resources, and context-awareness. We hence provide an updated account of recent developments on MEC from an energetic sustainability perspective, addressing the initial deployment of computing resources, their dynamic (re)allocation, as well as distributed and federated learning designs. In doing so, we highlight the energy aspects of these algorithms, advocating the need for energy-sustainable edge computing systems that are aligned with Sustainable Development Goals (SDGs) and the Paris agreement. To the best of our knowledge, this is the first work providing a systematic literature review on the efficient deployment and management of energy harvesting MEC, with special focus on the deployment, provisioning, and scheduling of computing tasks, including federated learning for distributed edge intelligence, toward making edge networks greener and more sustainable. At the end of the paper, open research avenues and challenges are identified for all the surveyed topics.},
    urldate = {2026-02-07},
    publisher = {arXiv},
    author = {Shalavi, Neda and Perin, Giovanni and Zanella, Andrea and Rossi, Michele},
    month = sep,
    year = {2022},
    note = {8 citations (Semantic Scholar/DOI) [2026-02-07]
arXiv:2209.14141 [cs]
TLDR: This is the first work providing a systematic literature review on the efficient deployment and management of energy harvesting MEC, with special focus on the deployment, provisioning, and scheduling of computing tasks, including federated learning for distributed edge intelligence, toward making edge networks greener and more sustainable.},
    keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Networking and Internet Architecture},
}
@inproceedings{song_simplified_2013,
    title = {A {Simplified} and {Accurate} {Model} of {Power}-{Performance} {Efficiency} on {Emergent} {GPU} {Architectures}},
    issn = {1530-2075},
    url = {https://ieeexplore.ieee.org/abstract/document/6569853},
    doi = {10.1109/IPDPS.2013.73},
    abstract = {Emergent heterogeneous systems must be optimized for both power and performance at exascale. Massive parallelism combined with complex memory hierarchies form a barrier to efficient application and architecture design. These challenges are exacerbated with GPUs as parallelism increases orders of magnitude and power consumption can easily double. Models have been proposed to isolate power and performance bottlenecks and identify their root causes. However, no current models combine simplicity, accuracy, and support for emergent GPU architectures (e.g. NVIDIA Fermi). We combine hardware performance counter data with machine learning and advanced analytics to model power-performance efficiency for modern GPU-based systems. Our performance counter based approach is simpler than previous approaches and does not require detailed understanding of the underlying architecture. The resulting model is accurate for predicting power (within 2.1\%) and performance (within 6.7\%) for application kernels on modern GPUs. Our model can identify power-performance bottlenecks and their root causes for various complex computation and memory access patterns (e.g. global, shared, texture). We measure the accuracy of our power and performance models on a NVIDIA Fermi C2075 GPU for more than a dozen CUDA applications. We show our power model is more accurate and robust than the best available GPU power models - multiple linear regression models MLR and MLR+. We demonstrate how to use our models to identify power-performance bottlenecks and suggest optimization strategies for high-performance codes such as GEM, a biomolecular electrostatic analysis application. We verify our power-performance model is accurate on clusters of NVIDIA Fermi M2090s and useful for suggesting optimal runtime configurations on the Keeneland supercomputer at Georgia Tech.},
    urldate = {2026-02-07},
    booktitle = {2013 {IEEE} 27th {International} {Symposium} on {Parallel} and {Distributed} {Processing}},
    author = {Song, Shuaiwen and Su, Chunyi and Rountree, Barry and Cameron, Kirk W.},
    month = may,
    year = {2013},
    note = {ISSN: 1530-2075},
    keywords = {Adaptation models, Graphics processing units, Kernel, Predictive models, Radiation detectors, Runtime, Training},
    pages = {673--686},
}
@article{fahad_comparative_2019,
    title = {A {Comparative} {Study} of {Methods} for {Measurement} of {Energy} of {Computing}},
    volume = {12},
    copyright = {http://creativecommons.org/licenses/by/3.0/},
    issn = {1996-1073},
    url = {https://www.mdpi.com/1996-1073/12/11/2204},
    doi = {10.3390/en12112204},
    abstract = {Energy of computing is a serious environmental concern and mitigating it is an important technological challenge. Accurate measurement of energy consu...},
    language = {en},
    number = {11},
    urldate = {2026-02-07},
    journal = {Energies},
    publisher = {publisher},
    author = {Fahad, Muhammad and Shahid, Arsalan and Manumachu, Ravi Reddy and Lastovetsky, Alexey},
    month = jun,
    year = {2019},
    note = {TLDR: It is shown that, owing to the nature of the deviations of the energy measurements provided by on-chip sensors from the ground truth, calibration can not improve the accuracy of the on- chip sensors to an extent that can allow them to be used in optimization of applications for dynamic energy.},
    keywords = {GPU, NVML, RAPL, Xeon Phi, energy efficiency, energy predictive models, multicore CPU, performance monitoring counters, power aensors, power meters},
}
@misc{nvidia_nvml,
    title = {{NVIDIA} {Management} {Library} ({NVML})},
    url = {https://developer.nvidia.com/management-library-nvml},
    language = {en-US},
    urldate = {2026-02-13},
    journal = {NVIDIA Developer},
}
@inproceedings{aslan_study_2022,
    title = {A {Study} on {Power} and {Energy} {Measurement} of {NVIDIA} {Jetson} {Embedded} {GPUs} {Using} {Built}-in {Sensor}},
    issn = {2521-1641},
    url = {https://ieeexplore.ieee.org/abstract/document/9919522/authors},
    doi = {10.1109/UBMK55850.2022.9919522},
    abstract = {Artificial intelligence (AI) has been shifted to the embedded devices known as edge devices. Component-level power is very important for the design and optimization of applications on edge devices to estimate energy consumption. Thus, accurate power measurements are needed for battery-powered systems. However, it is not straightforward. Because the behavior of a GPU is rather complex and not well documented. In this work, we report challenges getting power measurements using the built-in power sensor for an NVIDIA Jetson GPU device. We provide a method for true power and energy measurements of the kernels running on NVIDIA Jetson family GPUs.},
    urldate = {2026-02-07},
    booktitle = {2022 7th {International} {Conference} on {Computer} {Science} and {Engineering} ({UBMK})},
    author = {Aslan, Büşra and Yilmazer-Metin, Ayse},
    month = sep,
    year = {2022},
    note = {7 citations (Semantic Scholar/DOI) [2026-02-07]
ISSN: 2521-1641},
    keywords = {Artificial intelligence, Behavioral sciences, Computer science, Energy consumption, Energy measurement, Graphics processing units, Power measurement, dynamic power, edge devices, energy calculation, experimentation, internal power sensor, power monitoring, power profiling},
    pages = {1--6},
}
@inproceedings{holly_profiling_2020,
    title = {Profiling {Energy} {Consumption} of {Deep} {Neural} {Networks} on {NVIDIA} {Jetson} {Nano}},
    url = {https://ieeexplore.ieee.org/abstract/document/9290876},
    doi = {10.1109/IGSC51522.2020.9290876},
    abstract = {Improving the capabilities of embedded devices and accelerators for Deep Neural Networks (DNN) leads to a shift from cloud to edge computing. Especially for battery-powered systems, intelligent energy management is critical. In this work, we provide a measurement base for power estimation on NVIDIA Jetson devices. We analyze the effects of different CPU and GPU settings on power consumption, latency, and energy for complete DNNs as well as for individual layers. Furthermore, we provide optimal settings for minimal power and energy consumption for an NVIDIA Jetson Nano.},
    urldate = {2026-02-07},
    booktitle = {2020 11th {International} {Green} and {Sustainable} {Computing} {Workshops} ({IGSC})},
    author = {Holly, Stephan and Wendt, Alexander and Lechner, Martin},
    month = oct,
    year = {2020},
    note = {30 citations (Semantic Scholar/DOI) [2026-02-07]},
    keywords = {Energy consumption, Graphics processing units, Hardware, Jetson, NVIDIA, Neural networks, Power demand, Power measurement, Sensors, deep neural network, energy, latency, power, profiling},
    pages = {1--6},
}
@misc{noauthor_power_2023,
    title = {Power {Optimization} with {NVIDIA} {Jetson}},
    url = {https://developer.nvidia.com/blog/power-optimization-with-nvidia-jetson/},
    abstract = {When working with embedded systems such as the Jetson modules, you must optimize your application based on your power budget and compute resources. To avoid performance or even thermal throttling…},
    language = {en-US},
    urldate = {2026-02-07},
    journal = {NVIDIA Technical Blog},
    month = oct,
    year = {2023},
    keywords = {Domain/Documentation, Sec/Edge, Top/Edge/Orin, ⭐⭐⭐⭐⭐},
}
@article{gill_edge_2024,
    title = {Edge {AI}: {A} {Taxonomy}, {Systematic} {Review} and {Future} {Directions}},
    volume = {28},
    issn = {1573-7543},
    shorttitle = {Edge {AI}},
    url = {https://doi.org/10.1007/s10586-024-04686-y},
    doi = {10.1007/s10586-024-04686-y},
    abstract = {Edge Artificial Intelligence (AI) incorporates a network of interconnected systems and devices that receive, cache, process, and analyse data in close communication with the location where the data is captured with AI technology. Recent advancements in AI efficiency, the widespread use of Internet of Things (IoT) devices, and the emergence of edge computing have unlocked the enormous scope of Edge AI. The goal of Edge AI is to optimize data processing efficiency and velocity while ensuring data confidentiality and integrity. Despite being a relatively new field of research, spanning from 2014 to the present, it has shown significant and rapid development over the last five years. In this article, we present a systematic literature review for Edge AI to discuss the existing research, recent advancements, and future research directions. We created a collaborative edge AI learning system for cloud and edge computing analysis, including an in-depth study of the architectures that facilitate this mechanism. The taxonomy for Edge AI facilitates the classification and configuration of Edge AI systems while also examining its potential influence across many fields through compassing infrastructure, cloud computing, fog computing, services, use cases, ML and deep learning, and resource management. This study highlights the significance of Edge AI in processing real-time data at the edge of the network. Additionally, it emphasizes the research challenges encountered by Edge AI systems, including constraints on resources, vulnerabilities to security threats, and problems with scalability. Finally, this study highlights the potential future research directions that aim to address the current limitations of Edge AI by providing innovative solutions.},
    language = {en},
    number = {1},
    urldate = {2026-02-07},
    journal = {Cluster Computing},
    author = {Gill, Sukhpal Singh and Golec, Muhammed and Hu, Jianmin and Xu, Minxian and Du, Junhui and Wu, Huaming and Walia, Guneet Kaur and Murugesan, Subramaniam Subramanian and Ali, Babar and Kumar, Mohit and Ye, Kejiang and Verma, Prabal and Kumar, Surendra and Cuadrado, Felix and Uhlig, Steve},
    month = oct,
    year = {2024},
    note = {114 citations (Semantic Scholar/DOI) [2026-02-07]
TLDR: This study highlights the significance of Edge AI in processing real-time data at the edge of the network, and emphasizes the research challenges encountered by Edge AI systems, including constraints on resources, vulnerabilities to security threats, and problems with scalability.},
    keywords = {Artificial intelligence, Cloud computing, Edge AI, Edge computing, Machine learning},
    pages = {18},
}
@article{philipo_sustainable_2025,
    title = {Sustainable {AI}: {Emerging} {Trends}, {Impacts}, and {Future} {Challenges}},
    volume = {10},
    issn = {2377-3782},
    shorttitle = {Sustainable {AI}},
    url = {https://ieeexplore.ieee.org/document/11168278},
    doi = {10.1109/TSUSC.2025.3611272},
    abstract = {Sustainable AI considers both the environmental impact of AI technologies and their role in advancing global sustainability. This study provides a comprehensive analysis of emerging trends in sustainable AI, highlighting energy-efficient algorithms, green data centers, AI-driven resource management, and ethical governance. It emphasizes AI’s dual nature, its potential to drive climate action and socio-economic progress, alongside risks such as carbon emissions, e-waste, and digital inequality. The study underscores the need for interdisciplinary collaboration, inclusive policies, and standardized sustainability metrics to ensure responsible AI deployment. By identifying barriers and proposing strategies, it offers guidance to researchers, policymakers, and industry leaders on aligning AI with long-term environmental, ethical, and social objectives.},
    number = {6},
    urldate = {2026-02-10},
    journal = {IEEE Transactions on Sustainable Computing},
    author = {Philipo, Adamu Gaston and Ning, Huansheng and Sarwatt, Doreen Sebastian and Mohamed, Jumanne Ally and Yusufu, Afidhu Swaibu and Shi, Feifei and Urenje, Shepherd and Ding, Jianguo},
    month = nov,
    year = {2025},
    note = {0 citations (Semantic Scholar/DOI) [2026-02-10]},
    keywords = {AI governance, Artificial intelligence, Cooling, Data mining, Electronic waste, Energy consumption, Energy efficiency, Ethics, Green products, Measurement, Sustainable AI, Sustainable development, energy efficiency, environmental impact, ethical AI, green AI},
    pages = {1278--1291},
}
@misc{noauthor_jetson-stats_nodate,
    title = {jetson-stats},
    url = {https://rnext.it/jetson_stats/index.html},
    abstract = {jetson-stats is a package for monitoring and control your NVIDIA Jetson [Thor, Orin, Xavier, Nano, TX] series. Works with all NVIDIA Jetson ecosystem. Installing: jetson-stats can be installed with...},
    language = {en},
    urldate = {2026-02-07},
    journal = {jetson-stats},
    keywords = {⭐⭐⭐⭐},
}
@article{surianarayanan_survey_2023,
    title = {A {Survey} on {Optimization} {Techniques} for {Edge} {Artificial} {Intelligence} ({AI})},
    volume = {23},
    copyright = {http://creativecommons.org/licenses/by/3.0/},
    issn = {1424-8220},
    url = {https://www.mdpi.com/1424-8220/23/3/1279},
    doi = {10.3390/s23031279},
    abstract = {Artificial Intelligence (Al) models are being produced and used to solve a variety of current and future business and technical problems. Therefore, A...},
    language = {en},
    number = {3},
    urldate = {2026-02-07},
    journal = {Sensors},
    publisher = {publisher},
    author = {Surianarayanan, Chellammal and Lawrence, John Jeyasekaran and Chelliah, Pethuru Raj and Prakash, Edmond and Hewage, Chaminda},
    month = jan,
    year = {2023},
    note = {82 citations (Semantic Scholar/DOI) [2026-02-07]
TLDR: This paper is to dig deep and describe all kinds of model optimization at different levels and layers and highlighted the importance of having an enabling AI model optimization framework.},
    keywords = {AI model optimization, artificial intelligence, edge AI, energy efficient methods for edge AI, federated learning, optimization methods for edge AI},
}
@article{salehi_data-centric_2024,
    title = {Data-{Centric} {Green} {Artificial} {Intelligence}: {A} {Survey}},
    volume = {5},
    issn = {2691-4581},
    shorttitle = {Data-{Centric} {Green} {Artificial} {Intelligence}},
    url = {https://ieeexplore.ieee.org/document/10251541},
    doi = {10.1109/TAI.2023.3315272},
    abstract = {With the exponential growth of computational power and the availability of large-scale datasets in recent years, remarkable advancements have been made in the field of artificial intelligence (AI), leading to complex models and innovative applications. However, these models consume a significant unprecedented amount of energy, contributing to greenhouse gas emissions and a growing carbon footprint in the AI industry. In response, the concept of green AI has emerged, prioritizing energy efficiency and sustainability alongside accuracy and related measures. To this end, data-centric approaches are very promising to reduce the energy consumption of AI algorithms. This article presents a comprehensive overview of data-centric technologies and their impact on the energy efficiency of AI algorithms. Specifically, it focuses on methods that utilize training data in an efficient manner to improve the energy efficiency of AI algorithms. We have identified multiple data-centric approaches, such as active learning, knowledge transfer/sharing, dataset distillation, data augmentation, and curriculum learning that can contribute to the development of environmentally-friendly implementations of machine learning algorithms. Finally, the practical applications of these approaches are highlighted, and the challenges and future directions in the field are discussed.},
    number = {5},
    urldate = {2026-02-10},
    journal = {IEEE Transactions on Artificial Intelligence},
    author = {Salehi, Shirin and Schmeink, Anke},
    month = may,
    year = {2024},
    note = {44 citations (Semantic Scholar/DOI) [2026-02-10]},
    keywords = {Artificial intelligence, Computational efficiency, Computational modeling, Data-centric artificial intelligence (AI) (DCAI), Energy efficiency, Green products, Machine learning, Training, data-efficiency, energy-efficiency, green AI},
    pages = {1973--1989},
}
@article{kumar_towards_2025,
    title = {Towards {Green} {AI}: {A} {Novel} {Hybrid} {Filter} {Based} {AI} {Approach} for {Energy} {Efficient} {State} of {Charge} and {Energy} {Estimation} in {Li}-{Ion} {Batteries} {Under} {Various} {Drive} {Cycles}},
    volume = {61},
    issn = {1939-9367},
    shorttitle = {Towards {Green} {AI}},
    url = {https://ieeexplore.ieee.org/document/10922161},
    doi = {10.1109/TIA.2025.3550108},
    abstract = {Artificial intelligence models usually need a hefty dataset, which indeed requires more computational time, energy consumption, and laborious work to train the models to achieve accurate results. These models consume more energy to deliver output. Therefore, in this work proposes a novel filter technique (FT) based on an average reduction method. This approach improves the quality of the dataset, i.e., reduces volume and averages similar data to find the precision. This novel FT was incorporated with the convolutional neural network with bidirectional long short-term memory network (FT-CNN-Bi-LSTM) model and tested on four dynamic drive cycle circumstances. The key objective of this modelling technique is to enhance the performance and computational efficiency of the state of charge (SOC) and state of energy (SOE) of rechargeable battery systems such as lithium-ion batteries. A comparison examination was performed by proposed models configured with FT and traditional models, including FT-CNN, FT-LSTM, FT-BiLSTM, and FT-CNN-LSTM. The comparison examines SOC and SOE estimation efficacy and the model's capabilities and limitations. The FT-CNN-BiLSTM model was significantly tested on multiple dynamic drive cycles, including HWFET, LA92, UDDS, and US06. These drive cycle datasets were collected from our laboratory, where experiments were conducted at ambient temperature. The FT-CNN-Bi-LSTM approach achieved percentage improvements, 99.98\% of RMSE, 99.97\% of MSE, and 99.96\% of MAE. The significant percentage improvement in computational time and energy consumption ranges from 59 to 66\% and 61 to 68\%. These results demonstrate superior efficiency for rapid SOC and SOE estimation in all dynamic drive cycles.},
    number = {5},
    urldate = {2026-02-10},
    journal = {IEEE Transactions on Industry Applications},
    author = {Kumar, Deepak and Rizwan, M. and Panwar, Amrish K.},
    month = sep,
    year = {2025},
    note = {6 citations (Semantic Scholar/DOI) [2026-02-10]},
    keywords = {Accuracy, Artificial intelligence, Batteries, Computational modeling, Data models, Estimation, Filtering, Mathematical models, State of charge, Training, drive cycles, electric vehicles, lithium-ion batteries, state of charge (SOC) and energy estimation},
    pages = {7633--7645},
}
@article{kumar_towards_2026,
    title = {Towards {Green} {AI}: {Energy}-{Efficient} {State} of {Health} {Estimation} and {Degradation} {Analysis} of {Commercial} {Lithium}-{Ion} {Batteries} {Based} on {Deep} {Learning} and {Filter} {Technique} {Approach}},
    volume = {62},
    issn = {1939-9367},
    shorttitle = {Towards {Green} {AI}},
    url = {https://ieeexplore.ieee.org/document/11146426},
    doi = {10.1109/TIA.2025.3604739},
    abstract = {The redundant similar data points in the large datasets increase the dataset volume, storage, memory usage, training time, and computational resources, leading to significant inefficiency in the deep learning (DL) models. These inefficiencies reduce model performance and increase energy consumption. The existing DL-based approaches for state of health (SOH) estimation of lithium-ion batteries often face challenges such as high computational demands, low accuracy, and high energy consumption. These models require high energy for accurate results and contribute to higher electricity demand and carbon footprints. Therefore, this work proposes a novel filter technique (FT) based on the redundancy reduction approach. This approach improves the quality of the dataset, i.e., reduces dataset size, memory utilization, and energy consumption. This novel FT was incorporated with the gated recurrent unit (FT-GRU) model and tested on six different types of SOH datasets. The key objective of this approach is to enhance performance and lower energy consumption. A comparison examination was performed by proposed models configured with FT and traditional models to investigate the SOH estimation effectiveness and the model’s abilities and limits. The FT-GRU model was significantly tested on laboratory and publicly available NASA dynamic datasets, including B1, B2, B05, B06, B07 and B18. The B1 and B2 datasets were collected from our laboratory at room temperature. The FT-GRU approach achieved percentage improvements of 75.96\% of RMSE, 99.49\% of MSE, 93.07\% of MAE, and computational time and energy consumption ranges from 25\% to 70.76\% and 69.50\% to 82.38\%, respectively, in comparison to the other existing approaches.},
    number = {2},
    urldate = {2026-02-10},
    journal = {IEEE Transactions on Industry Applications},
    author = {Kumar, Deepak and Ahmed, Mujeeb and Jamil, Majid and Rizwan, M. and Panwar, Amrish K.},
    month = mar,
    year = {2026},
    note = {0 citations (Semantic Scholar/DOI) [2026-02-10]},
    keywords = {Accuracy, Adaptation models, Artificial intelligence, Computational efficiency, Computational modeling, Degradation, Energy consumption, Energy efficiency, Estimation, Green AI, Training, degradation analysis, energy efficiency DL models, lithium-ion batteries, state of health},
    pages = {3009--3022},
}
@article{kim_eco-friendly_2023,
    title = {Eco-{Friendly} {Low} {Resource} {Security} {Surveillance} {Framework} {Toward} {Green} {AI} {Digital} {Twin}},
    volume = {27},
    issn = {1558-2558},
    url = {https://ieeexplore.ieee.org/document/9932580},
    doi = {10.1109/LCOMM.2022.3218050},
    abstract = {Most intelligent systems focused on how to improve performance including accuracy, processing speed with a massive number of data sets and those performance-biased intelligent systems, Red AI systems, have been applied to digital twin in smart cities. On the other hand, it is highly reasonable to consider Green AI features covering environmental, economic, social costs for advanced digital twin services. In this letter, we propose eco-friendly low resource security surveillance toward Green AI-enabled digital twin service, which provides eco-friendly security by the active participation of low resource devices. And, we formally define a problem whose objective is to maximize the participation of low source or reusable devices such that reusable surveillance borders are created within security district. Also, a dense sub-district with low resource devices priority completion scheme is proposed to resolve the problem. Then, the devised method is performed by expanded simulations and the achieved result is evaluated with demonstrated discussions.},
    number = {1},
    urldate = {2026-02-10},
    journal = {IEEE Communications Letters},
    author = {Kim, Hyunbum and Ben-Othman, Jalel},
    month = jan,
    year = {2023},
    note = {31 citations (Semantic Scholar/DOI) [2026-02-10]},
    keywords = {Artificial intelligence, Digital twins, Eco-friendly, Green products, Object recognition, Performance evaluation, Security, Surveillance, digital twin, low resource, security, surveillance},
    pages = {377--380},
}
@article{kumar_advances_2023,
    title = {Advances in {Batteries}, {Battery} {Modeling}, {Battery} {Management} {System}, {Battery} {Thermal} {Management}, {SOC}, {SOH}, and {Charge}/{Discharge} {Characteristics} in {EV} {Applications}},
    volume = {11},
    issn = {2169-3536},
    url = {https://ieeexplore.ieee.org/document/10258162},
    doi = {10.1109/ACCESS.2023.3318121},
    abstract = {The second-generation hybrid and Electric Vehicles are currently leading the paradigm shift in the automobile industry, replacing conventional diesel and gasoline-powered vehicles. The Battery Management System is crucial in these electric vehicles and also essential for renewable energy storage systems. This review paper focuses on batteries and addresses concerns, difficulties, and solutions associated with them. It explores key technologies of Battery Management System, including battery modeling, state estimation, and battery charging. A thorough analysis of numerous battery models, including electric, thermal, and electro-thermal models, is provided in the article. Additionally, it surveys battery state estimations for a charge and health. Furthermore, the different battery charging approaches and optimization methods are discussed. The Battery Management System performs a wide range of tasks, including as monitoring voltage and current, estimating charge and discharge, equalizing and protecting the battery, managing temperature conditions, and managing battery data. It also looks at various cell balancing circuit types, current and voltage stressors, control reliability, power loss, efficiency, as well as their advantages and disadvantages. The paper also discusses research gaps in battery management systems.},
    urldate = {2026-02-10},
    journal = {IEEE Access},
    author = {Kumar, R. Ranjith and Bharatiraja, C. and Udhayakumar, K. and Devakirubakaran, S. and Sekar, K. Sathiya and Mihet-Popa, Lucian},
    year = {2023},
    keywords = {Batteries, Battery management systems, Discharges (electric), Electric vehicle, Electric vehicles, Kalman filters, Renewable energy sources, State of charge, Thermal management, Voltage control, battery management, battery modelling, battery thermal management system, cell balancing, state of charge, state of health},
    pages = {105761--105809},
}
@article{haraz_state--health_2024,
    title = {State-of-{Health} and {State}-of-{Charge} {Estimation} in {Electric} {Vehicles} {Batteries}: {A} {Survey} on {Machine} {Learning} {Approaches}},
    volume = {12},
    issn = {2169-3536},
    shorttitle = {State-of-{Health} and {State}-of-{Charge} {Estimation} in {Electric} {Vehicles} {Batteries}},
    url = {https://ieeexplore.ieee.org/document/10736577},
    doi = {10.1109/ACCESS.2024.3486989},
    abstract = {Precise estimation of both state-of-charge (SoC) and state-of-health (SoH) is crucial for optimizing electric vehicle (EV) performance and enhancing the battery lifetime, safety, and reliability, where machine learning (ML) plays a vital role in this regard. While existing surveys explore ML applications in EVs, they often need to address ML approaches for SoC and SoH estimation. This paper bridges this gap by comprehensively reviewing how ML is utilized for SoC and SoH estimation, analyzing their strengths and weaknesses across different battery chemistries. Our review offers a systematic breakdown of critical areas: fundamental concepts and functionalities of prominent ML techniques for estimating SoC and SoH, a comparative evaluation of ML techniques applied to diverse EV battery types, an exploration of SoC and SoH estimation using modeling approaches within EV battery systems, and the critical role of dataset quality and model evaluation criteria. Moreover, this paper addresses ML tools developed for lithium-ion batteries (LiBs), image processing applications in EV batteries, and an in-depth investigation of the system model for ML-based SoH and SoC estimation. Furthermore, we present key concepts and methods for SoH and SoC estimation utilizing ML, compare input features, metrics, hyperparameters, and datasets, and demonstrate ML-based system models for EV battery estimation. By conducting this thorough analysis, we aim to close the existing gap and stimulate future progress in ML for SoH and SoC estimation, primarily focusing on LiBs across different EV applications.},
    urldate = {2026-02-10},
    journal = {IEEE Access},
    author = {Haraz, Aya and Abualsaud, Khalid and Massoud, Ahmed},
    year = {2024},
    note = {TLDR: This paper comprehensively reviews how ML is utilized for SoC and SoH estimation, analyzing their strengths and weaknesses across different battery chemistries, and presents key concepts and methods for SoH and SoC estimation utilizing ML.},
    keywords = {Batteries, Battery management systems, Estimation, Lead, Lithium-ion batteries, Machine learning, Maximum likelihood estimation, Monitoring, Safety, Surveys, electric vehicles, lithium-ion batteries, state-of-charge, state-of-health, ⭐⭐⭐⭐⭐},
    pages = {158110--158139},
}
@article{seifoddini_comparative_2025,
    title = {A {Comparative} {Study} of the {DEKF} and {DUKF} for {Battery} {SOC} and {SOH} {Estimation}},
    volume = {11},
    copyright = {http://creativecommons.org/licenses/by/3.0/},
    issn = {2313-0105},
    url = {https://www.mdpi.com/2313-0105/11/11/410},
    doi = {10.3390/batteries11110410},
    abstract = {The accurate estimation of the state of charge (SOC) and state of health (SOH) is essential for the safety and reliability of electric vehicle batteri...},
    language = {en},
    number = {11},
    urldate = {2026-02-07},
    journal = {Batteries},
    publisher = {publisher},
    author = {Seifoddini, Arash and Miretti, Federico and Misul, Daniela Anna},
    month = nov,
    year = {2025},
    note = {0 citations (Semantic Scholar/DOI) [2026-02-07]},
    keywords = {battery management system, dual Kalman filter, electric vehicle batteries, extended Kalman filter, state of charge, state of health, unscented Kalman filter},
}
@article{sui_review_2021,
    title = {A review of non-probabilistic machine learning-based state of health estimation techniques for {Lithium}-ion battery},
    volume = {300},
    issn = {0306-2619},
    url = {https://www.scopus.com/pages/publications/85110186845},
    doi = {10.1016/j.apenergy.2021.117346},
    abstract = {Lithium-ion batteries are used in a wide range of applications including energy storage systems, electric transportations, and portable electronic devices. Accurately obtaining the batteries’ state of health (SOH) is critical to prolong the service life of the battery and ensure the safe and reliable operation of the system. Machine learning (ML) technology has attracted increasing attention due to its competitiveness in studying the behavior of complex nonlinear systems. With the development of big data and cloud computing, ML technology has a big potential in battery SOH estimation. In this paper, the five most studied types of ML algorithms for battery SOH estimation are systematically reviewed. The basic principle of each algorithm is rigorously derived followed by flow charts with a unified form, and the advantages and applicability of different methods are compared from a theoretical perspective. Then, the ML-based SOH estimation methods are comprehensively compared from following three aspects: the estimation performance of various algorithms under five performance metrics, the publication trend obtained by counting the publications in the past ten years, and the training modes considering the feature extraction and selection methods. According to the comparison results, it can be concluded that amongst these methods, support vector machine and artificial neural network algorithms are still research hotspots. Deep learning has great potential in estimating battery SOH under complex aging conditions especially when big data is available. Moreover, the ensemble learning method provides an emerging alternative trading-off between data size and accuracy. Finally, the outlooks of the research on future ML-based battery SOH estimation methods are closed, hoping to provide some inspiration when applying ML methods to battery SOH estimation.},
    number = {117346},
    urldate = {2026-02-07},
    journal = {Applied Energy},
    author = {Sui, Xin and He, Shan and Vilsen, Søren Byg and Meng, Jinhao and Teodorescu, Remus and Stroe, Daniel-Ioan},
    month = oct,
    year = {2021},
    note = {TLDR: The five most studied types of ML algorithms for battery SOH estimation are systematically reviewed and it can be concluded that amongst these methods, support vector machine and artificial neural network algorithms are still research hotspots.},
    keywords = {Battery Management System, Deep Learning, Health monitoring, Lithium-ion battery, Machine learning, State of health},
}
@article{xu_novel_2021,
    title = {A novel adaptive dual extended {Kalman} filtering algorithm for the {Li}-ion battery state of charge and state of health co-estimation},
    volume = {45},
    copyright = {© 2021 John Wiley \& Sons Ltd.},
    issn = {1099-114X},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/er.6719},
    doi = {10.1002/er.6719},
    abstract = {Accurate prediction of the state of health (SOH) of Li-ion battery has an important role in the estimation of battery state of charge (SOC), which can not only improve the efficiency of battery usage but also ensure its safety performance. The battery capacity will decrease with the increase of charge and discharge times, while the internal resistance will become larger, which will affect battery management. The capacity attenuation characteristics of Li-ion batteries are analyzed by aging experiment. Based on the equivalent circuit model and online parameter identification, a novel adaptive dual extended Kalman filter algorithm is proposed to consider the influence of the battery SOH on the estimation of the battery SOC, and the SOC and SOH of the Li-ion battery are estimated collaboratively. The feasibility and accuracy of the model and algorithm are verified by experiments. The results show that the algorithm has good convergence and tracking. The maximum error in the estimation of the SOC is 2.03\%, and the maximum error of the Ohmic resistance is 15.3\%. It can better evaluate the SOH and SOC of Li-ion battery and reduce the dependence on experimental data, providing a reference for the efficient management of Li-ion batteries.},
    language = {en},
    number = {10},
    urldate = {2026-02-07},
    journal = {International Journal of Energy Research},
    author = {Xu, Wenhua and Wang, Shunli and Jiang, Cong and Fernandez, Carlos and Yu, Chunmei and Fan, Yongcun and Cao, Wen},
    year = {2021},
    note = {74 citations (Semantic Scholar/DOI) [2026-02-07]
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/er.6719},
    keywords = {Li-ion battery, dual extended Kalman filter, online parameter identification, state of charge, state of health},
    pages = {14592--14602},
}
@article{barros_adaptive_2025,
    title = {Adaptive {Extended} {Kalman} {Filtering} for {Battery} {State} of {Charge} {Estimation} on {STM32}},
    volume = {17},
    issn = {1943-0671},
    url = {https://ieeexplore.ieee.org/abstract/document/10740057},
    doi = {10.1109/LES.2024.3489352},
    abstract = {Accurate and computationally light algorithms for estimating the state of charge (SoC) of a battery’s cells are crucial for effective battery management on embedded systems. In this letter, we propose an adaptive extended Kalman filter (AEKF) for SoC estimation using a covariance adaptation technique based on maximum likelihood estimation—a novelty in this domain. Furthermore, we tune a key design parameter—the estimation window size—to obtain an optimal memory-performance tradeoff, and experimentally demonstrate our solution achieves superior estimation accuracy with respect to existing alternative methods. Finally, we present a fully custom implementation of the AEKF for a general-purpose low-cost STM32 microcontroller, showing it can be deployed with minimal computational requirements adequate for real-world usage.},
    number = {3},
    urldate = {2026-02-07},
    journal = {IEEE Embedded Systems Letters},
    author = {Barros, António and Peretti, Edoardo and Fabroni, Davide and Carrera, Diego and Fragneto, Pasqualina and Boracchi, Giacomo},
    month = jun,
    year = {2025},
    note = {2 citations (Semantic Scholar/DOI) [2026-02-07]
TLDR: This letter proposes an adaptive extended Kalman filter (AEKF) for SoC estimation using a covariance adaptation technique based on maximum likelihood estimation—a novelty in this domain.},
    keywords = {Adaptive extended Kalman filter (AEKF), Batteries, Computational modeling, Covariance matrices, Data structures, Integrated circuit modeling, Kalman filters, Load modeling, Maximum likelihood estimation, STM32, State of charge, Voltage measurement, embedded implementation, state of charge (SoC) estimation},
    pages = {160--163},
}
@article{wang_hierarchical_2023,
    title = {A hierarchical adaptive extended {Kalman} filter algorithm for lithium-ion battery state of charge estimation},
    volume = {62},
    issn = {2352-152X},
    url = {https://www.sciencedirect.com/science/article/pii/S2352152X23002281},
    doi = {10.1016/j.est.2023.106831},
    abstract = {State of charge (SOC) is a key state in the battery management system (BMS). For a second-order equivalent circuit model (ECM), a hierarchical adaptive extended Kalman filter (HAEKF) algorithm is investigated for SOC estimation. Firstly, by a Sage-Husa estimator updating process noise online, an adaptive EKF algorithm is adopted to improve SOC estimation. Then, by the hierarchical identification principle, the circuit state equation model is decomposed into two fictitious state equation submodels with different sampling rates to solve the fast/slow dynamic problem existing in the two different resistance-capacitance (RC) networks, respectively, in which states are alternatively estimated by the HAEKF algorithm. In the two dual-rate submodels, the faster sampling rate for the fast dynamic submodel ensures the timely state updates, the slower sampled rate for the slow dynamic submodel reduces the computational burden and error accumulation. Finally, under the urban dynamometer driving schedule (UDDS) test condition, experimental results verify that the HAEKF algorithm has high SOC estimation accuracy, lower computational cost, and strong robustness under left biased measurement noise variance.},
    urldate = {2026-02-07},
    journal = {Journal of Energy Storage},
    author = {Wang, Dongqing and Yang, Yan and Gu, Tianyu},
    month = jun,
    year = {2023},
    note = {98 citations (Semantic Scholar/DOI) [2026-02-07]},
    keywords = {Extended Kalman filter (EKF), Hierarchical identification principle, Lithium-ion battery, Sage-Husa estimator, State of charge (SOC)},
    pages = {106831},
}
@article{yang_parameter_2021,
    title = {A parameter adaptive method for state of charge estimation of lithium-ion batteries with an improved extended {Kalman} filter},
    volume = {11},
    copyright = {2021 The Author(s)},
    issn = {2045-2322},
    url = {https://www.nature.com/articles/s41598-021-84729-1},
    doi = {10.1038/s41598-021-84729-1},
    abstract = {An accurate state of charge (SOC) estimation in battery management systems (BMS) is of crucial importance to guarantee the safe and effective operation of automotive batteries. However, the BMS consistently suffers from inaccuracy of SOC estimation. Herein, we propose a SOC estimation approach with both high accuracy and robustness based on an improved extended Kalman filter (IEKF). An equivalent circuit model is established, and the simulated annealing-particle swarm optimization (SA-PSO) algorithm is used for offline parameter identification. Furthermore, improvements have been made with noise adaptation, a fading filter and a linear-nonlinear filtering based on the traditional EKF method, and rigorous mathematical proof has been carried out accordingly. To deal with model mismatch, online parameter identification is achieved by a dual Kalman filter. Finally, various experiments are performed to validate the proposed IEKF. Experimental results show that the IEKF algorithm can reduce the error to 2.94\% under dynamic stress test conditions, and robustness analysis is verified with noise interference, hence demonstrating its practicability for extending to state estimation of battery packs applied in real-world operating conditions.},
    language = {en},
    number = {1},
    urldate = {2026-02-07},
    journal = {Scientific Reports},
    publisher = {Nature Publishing Group},
    author = {Yang, Shichun and Zhou, Sida and Hua, Yang and Zhou, Xinan and Liu, Xinhua and Pan, Yuwei and Ling, Heping and Wu, Billy},
    month = mar,
    year = {2021},
    note = {TLDR: This work proposes a SOC estimation approach with both high accuracy and robustness based on an improved extended Kalman filter (IEKF) and demonstrates its practicability for extending to state estimation of battery packs applied in real-world operating conditions.},
    keywords = {Batteries, Electrical and electronic engineering},
    pages = {5805},
}
@article{fahmy_state_2024,
    title = {State of health estimation of lithium-ion battery using dual adaptive unscented {Kalman} filter and {Coulomb} counting approach},
    volume = {88},
    issn = {2352-152X},
    url = {https://www.sciencedirect.com/science/article/pii/S2352152X24011423},
    doi = {10.1016/j.est.2024.111557},
    abstract = {Several charging and discharging processes of lithium-ion batteries (LIBs) can lead to a battery fading and degradation effect. This may cause sudden faults, leakages, and explosions. As a result, it is highly important to estimate the state of health (SoH) of the battery to avoid any battery problems. This article proposes a novel hybrid dual adaptive unscented Kalman filter (DAUKF)-Coulomb counting approach (CCA) to efficiently state of charge (SoC) and SoH estimation of LIBs. The Gazelle optimization algorithm (GOA) is utilized in identifying LIB model parameters under various SoC conditions. It is used for minimizing the integral squared error between measured and estimated battery voltages. The battery model includes loading, fading, and various dynamic conditions. The GOA-based model has better results than other models by {\textgreater}8 \%. The proposed hybrid DAUKF-CCA is compared with the dual adaptive extended Kalman filter (DAEKF)-CCA and other multiple algorithms. The fitness function consists of integral squared error between estimated and measured SoC of LIBs. The simulation results of the DAUKF-CCA are verified by a comparison with the measurement results performed using commercial Panasonic LiBs. The SoC results using the DAUKF-CCA are very close to the measurement results and the error is {\textless}1 \%. The proposed DAUKF-CCA can ensure that the SoC and SoH estimation of LIBs is efficiently achieved.},
    urldate = {2026-02-07},
    journal = {Journal of Energy Storage},
    author = {Fahmy, Hend M. and Hasanien, Hany M. and Alsaleh, Ibrahim and Ji, Haoran and Alassaf, Abdullah},
    month = may,
    year = {2024},
    keywords = {Adaptive Kalman filters, Battery modeling, Lithium-ion battery, Optimization algorithms, State of health},
    pages = {111557},
}
@article{xie_state_2023,
    title = {State of charge estimation of lithium-ion battery based on extended {Kalman} filter algorithm},
    volume = {11},
    issn = {2296-598X},
    url = {https://www.frontiersin.org/journals/energy-research/articles/10.3389/fenrg.2023.1180881/full},
    doi = {10.3389/fenrg.2023.1180881},
    abstract = {Due to excellent power and energy density, low self-discharge and long life, lithium-ion battery plays an important role in many fields. Directed against the complexity of above noises and the strong sensitivity of the common Kalman filter algorithm to noises, the state of charge estimation of lithium-ion battery based on extended Kalman filter algorithm is investigated in this paper. Based on the second-order resistor-capacitance equivalent circuit model, the battery model parameters are identified using the MATLAB/Simulink software. A battery parameter test platform is built to test the charge-discharge efficiency, open-circuit voltage and state of charge relationship curve, internal resistance and capacitance of the individual battery are tested. The simulation and experimental results of terminal voltage for lithium-ion battery is compared to verify the effectiveness of this method. In addition, the general applicability of state of charge estimation algorithm for the battery pack is explored. The ampere-hour integral method combined with the battery modeling is used to estimate the state of charge of lithium-ion battery. The comparison of extended Kalman filter algorithm between experimental results and simulation estimated results is obtained to verify the accuracy. The extended Kalman filter algorithm proposed in this study not only establishes the theoretical basis for the condition monitoring but also provides the safe guarantee for the engineering application of lithium-ion battery.},
    language = {English},
    urldate = {2026-02-07},
    journal = {Frontiers in Energy Research},
    publisher = {Frontiers},
    author = {Xie, Jiamiao and Wei, Xingyu and Bo, Xiqiao and Zhang, Peng and Chen, Pengyun and Hao, Wenqian and Yuan, Meini},
    month = may,
    year = {2023},
    keywords = {Extended Kalman filter algorithm, MATLAB/ simulink, Second-order resistor-capacitance (RC) equivalent circuit model, State of charge (SOC), lithium-ion battery},
}
@article{hochreiter_long_1997,
    title = {Long {Short}-{Term} {Memory}},
    volume = {9},
    issn = {0899-7667},
    url = {https://ieeexplore.ieee.org/abstract/document/6795963},
    doi = {10.1162/neco.1997.9.8.1735},
    abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
    number = {8},
    urldate = {2026-02-07},
    journal = {Neural Computation},
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    month = nov,
    year = {1997},
    note = {TLDR: A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units.},
    keywords = {⭐⭐⭐⭐},
    pages = {1735--1780},
}
@misc{cho_learning_2014,
    title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
    url = {http://arxiv.org/abs/1406.1078},
    doi = {10.48550/arXiv.1406.1078},
    abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
    urldate = {2026-02-07},
    publisher = {arXiv},
    author = {Cho, Kyunghyun and Merrienboer, Bart van and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
    month = sep,
    year = {2014},
    note = {arXiv:1406.1078 [cs]},
    keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}
@article{lu_batt-predic_2022,
    title = {Battery degradation prediction against uncertain future conditions with recurrent neural network enabled deep learning},
    volume = {50},
    issn = {2405-8297},
    url = {https://www.sciencedirect.com/science/article/pii/S2405829722002446},
    doi = {10.1016/j.ensm.2022.05.007},
    abstract = {Accurate degradation trajectory and future life are the key information of a new generation of intelligent battery and electrochemical energy storage systems. It is very challenging to obtain accurate predictions against uncertain application conditions by using only a few known historical data. In this article, we extend the widely studied remaining useful life (RUL) prediction to the prediction of charge and discharge capacity trajectories under both fixed and random future operating conditions. This is achieved by developing a general deep learning framework cored by recurrent neural network (RNN) which integrates future current plan and few early capacity-voltage data as inputs. As a case study, we have experimented with 77 commercial batteries cycled under fixed and random operating conditions. We demonstrate that the median root mean square error (RMSE) of prediction can be within 2.4\% for NMC/graphite batteries and 2.3\% for LFP/graphite batteries by using 3.8\% of the whole life data only. Compared with the existing methods, the proposed framework predicts more accurately and has a very balanced performance for both fixed and random future conditions. This work highlights the promise of actively forecasting the future of batteries based on RNN.},
    urldate = {2026-01-30},
    journal = {Energy Storage Materials},
    author = {Lu, Jiahuan and Xiong, Rui and Tian, Jinpeng and Wang, Chenxu and Hsu, Chia-Wei and Tsou, Nien-Ti and Sun, Fengchun and Li, Ju},
    month = sep,
    year = {2022},
    note = {198 citations (Semantic Scholar/DOI) [2026-01-30]
TLDR: This work highlights the promise of actively forecasting the future of batteries based on RNN by developing a general deep learning framework cored by recurrent neural network (RNN) which integrates future current plan and few early capacity-voltage data as inputs.},
    keywords = {Alg/ml/GRU, Alg/ml/LSTM, Domain/RUL, Domain/SOH, Hw/gpu, Metric/accuracy, Sec/Background, Sec/ML, Top/PC, ⭐⭐⭐⭐},
    pages = {139--151},
}
@article{xu_lstm-based_2024,
    title = {{LSTM}-based estimation of lithium-ion battery {SOH} using data characteristics and spatio-temporal attention},
    volume = {19},
    issn = {1932-6203},
    url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC11671002/},
    doi = {10.1371/journal.pone.0312856},
    abstract = {As the primary power source for electric vehicles, the accurate estimation of the State of Health (SOH) of lithium-ion batteries is crucial for ensuring the reliable operation of the power system. Long Short-Term Memory (LSTM), a special type of recurrent neural network, achieves sequence information estimation through a gating mechanism. However, traditional LSTM-based SOH estimation methods do not account for the fact that the degradation sequence of battery SOH exhibits trend-like nonlinearity and significant dynamic variations between samples. Therefore, this paper proposes an LSTM-based lithium-ion SOH estimation method incorporating data characteristics and spatio-temporal attention. First, considering the trend-like nonlinearity of the degradation sequence, which is initially gradual and then rapid, input features are filtered and divided into trend and non-trend features. Then, to address the significant dynamic variations between samples, especially for capacity regeneration,a spatio-temporal attention mechanism is designed to extract spatio-temporal features from multidimensional non-trend features. Subsequently, an LSTM model is built with trend features, spatio-temporal features, and actual capacity as inputs to estimate capacity. Finally, the model is trained and tested on different datasets. Experimental results demonstrate that the proposed method outperforms traditional methods in terms of SOH estimation accuracy and robustness.},
    number = {12},
    urldate = {2026-02-07},
    journal = {PLOS ONE},
    author = {Xu, Gengchen and Xu, Jingyun and Zhu, Yifan},
    month = dec,
    year = {2024},
    note = {14 citations (Semantic Scholar/DOI) [2026-02-07]
TLDR: Experimental results demonstrate that the proposed LSTM-based lithium-ion SOH estimation method outperforms traditional methods in terms of SOH estimation accuracy and robustness.},
    pages = {e0312856},
}
@article{zhang_lithium-ion_2025,
    title = {Lithium-{Ion} {Batteries} state of health estimation based on optimized {TCN}-{GRU}-{WNN}},
    volume = {13},
    issn = {2352-4847},
    url = {https://www.sciencedirect.com/science/article/pii/S2352484725000927},
    doi = {10.1016/j.egyr.2025.02.007},
    abstract = {The exponential growth of the new energy vehicle sector has underscored the critical importance of precise state of health estimation for Lithium-Ion Batteries, which constitute the fundamental component of power systems in these vehicles. Addressing the limitations of conventional physical model-based estimation techniques, characterized by high computational demands and suboptimal real-time performance, this study introduces an innovative hybrid model. This model integrates a Genetic Algorithm (GA)-optimized Temporal Convolutional Network (TCN), Gated Recurrent Unit (GRU), and Wavelet Neural Network (WNN) to assess the state of health (SOH) of Lithium-Ion Batteries. Within this framework, the TCN extracts critical temporal features from battery operation data through temporal convolution operations, the GRU captures long-term dependencies of the data sequences, and the WNN performs multi-scale signal decomposition through wavelet transforms to enhance the model's nonlinear mapping capability. To maximize model efficacy, GA was utilized to conduct a comprehensive search for optimal parameter configurations. The results show that this hybrid method can estimate the SOH of lithium-ion batteries with high accuracy and robustness, and the average estimation error is significantly reduced to less than 1 \%, thus offering valuable support for battery health management and contributing to enhanced safety and efficiency in both new energy vehicles and energy storage systems.},
    urldate = {2026-02-07},
    journal = {Energy Reports},
    author = {Zhang, Nan and Li, Jing and Ma, Yunfeng and Wu, Kunzhen},
    month = jun,
    year = {2025},
    keywords = {Deep learning, Lithium-Ion Batteries, NASA dataset, State of health},
    pages = {2502--2515},
}
@article{chen_towards_2025,
    title = {Towards practical data-driven battery state of health estimation: {Advancements} and insights targeting real-world data},
    volume = {110},
    issn = {2095-4956},
    shorttitle = {Towards practical data-driven battery state of health estimation},
    url = {https://www.sciencedirect.com/science/article/pii/S2095495625005741},
    doi = {10.1016/j.jechem.2025.07.022},
    abstract = {Accurate state of health (SOH) estimation is a cornerstone for ensuring the safety, performance and longevity of lithium-ion batteries, especially in electric vehicle (EV) applications. While numerous studies have demonstrated the significant advantages of data-driven methods in SOH estimation, most rely on laboratory-standardized test data. This raises concerns about the generalization and robustness of the models under real-world operating conditions, where batteries undergo irregular driving patterns, incomplete charging cycles, and unpredictable environments. Notably, real-world EV data reflects the coupling between battery aging characteristics and actual operating conditions, providing an unprecedented perspective for developing SOH estimation models. This review provides a comprehensive and systematic overview of data-driven SOH estimation using real-world data, a topic that has received increasing attention but lacks a consolidated research framework. The paper begins by reviewing the established SOH estimation methodologies and points out the specific challenges arising from the transition to real-world data. It then probes practical issues across the pipeline: data pre-processing for anomalies, solutions for the lack of labels, feature extraction from complex operating data, machine learning model construction, and performance evaluation across various system deployments. Key insights are presented on how to handle noisy, unlabeled, and heterogeneous data using robust modeling strategies. Moreover, a valuable extension focusing on applying the advancements to battery reuse and recycling is discussed, with the goal of developing a whole lifecycle health diagnosis framework. The paper concludes with promising prospects, encompassing open-source standardized dataset establishment, weakly supervised learning, physics-reinforced modeling, real-world deployment, and advanced sensing technology, emphasizing that real-world data makes the transition of data-driven methods from theoretical validation to industrial deployment promising. This paper aims to assist researchers and practitioners in navigating the complexities of real-world SOH estimation, accelerating the collaborative innovation and industrial adoption in battery health management.},
    urldate = {2026-02-07},
    journal = {Journal of Energy Chemistry},
    author = {Chen, Hongxu and Chen, Ying and Sun, Changzheng and Huo, Liping and Zhang, Wenjun and Shen, Ping and Huang, Lvwei and Luan, Weiling and Chen, Haofeng},
    month = nov,
    year = {2025},
    note = {2 citations (Semantic Scholar/DOI) [2026-02-07]},
    keywords = {Battery health management, Data-driven, Electric vehicle (EV), Lithium-ion battery, Real-world application, State of health (SOH)},
    pages = {657--680},
}
@article{lu_deep_2023,
    title = {Deep learning to estimate lithium-ion battery state of health without additional degradation experiments},
    volume = {14},
    copyright = {2023 The Author(s)},
    issn = {2041-1723},
    url = {https://www.nature.com/articles/s41467-023-38458-w},
    doi = {10.1038/s41467-023-38458-w},
    abstract = {State of health is a critical state which evaluates the degradation level of batteries. However, it cannot be measured directly but requires estimation. While accurate state of health estimation has progressed markedly, the time- and resource-consuming degradation experiments to generate target battery labels hinder the development of state of health estimation methods. In this article, we design a deep-learning framework to enable the estimation of battery state of health in the absence of target battery labels. This framework integrates a swarm of deep neural networks equipped with domain adaptation to produce accurate estimation. We employ 65 commercial batteries from 5 different manufacturers to generate 71,588 samples for cross-validation. The validation results indicate that the proposed framework can ensure absolute errors of less than 3\% for 89.4\% of samples (less than 5\% for 98.9\% of samples), with a maximum absolute error of less than 8.87\% in the absence of target labels. This work emphasizes the power of deep learning in precluding degradation experiments and highlights the promise of rapid development of battery management algorithms for new-generation batteries using only previous experimental data.},
    language = {en},
    number = {1},
    urldate = {2026-02-07},
    journal = {Nature Communications},
    publisher = {Nature Publishing Group},
    author = {Lu, Jiahuan and Xiong, Rui and Tian, Jinpeng and Wang, Chenxu and Sun, Fengchun},
    month = may,
    year = {2023},
    note = {258 citations (Semantic Scholar/DOI) [2026-02-07]
TLDR: A deep-learning framework is designed to enable the estimation of Li-ion battery state of health in the absence of target battery labels that integrates a swarm of deep neural networks equipped with domain adaptation to produce accurate estimation.},
    keywords = {Batteries, Electrical and electronic engineering, Energy modelling},
    pages = {2760},
}
@article{lu_battery_2022,
    title = {Battery {Degradation} {Dataset} ({Fixed} {Current} {Profiles}\&{Arbitrary} {Uses} {Profiles})},
    volume = {3},
    url = {https://data.mendeley.com/datasets/kw34hhw7xg/3},
    doi = {10.17632/kw34hhw7xg.3},
    abstract = {77 nominally identical high-energy 18650 lithium-ion batteries are cycled with fixed or arbitrary uses current profiles. 22 batteries are cycled with fixed current profiles of charge current (1C, 2C, or 3C) and discharge current (1C, 2C, or 3C). 55 batteries are cycled with arbitrary uses profiles of charge current (obeys a uniform distribution among 1C, 2C, or 3C, and changes randomly every 5 cycles) and a specified discharge current (3C). All the battery degradation tests were carried out by the Advanced Energy Storage and Application (AESA) Group at Beijing Institute of Technology. If you make use of our data, please cite our dataset directly using its DOI, as well as the following papers: [1] Lu, J., Xiong, R., Tian, J., Wang, C., Hsu, C. W., Tsou, N. T., Sun, F., \& Li, J. (2022). Battery Degradation Prediction Against Uncertain Future Conditions with Recurrent Neural Network Enabled Deep Learning. Energy Storage Materials, 50, 139-151. https://doi.org/10.1016/j.ensm.2022.05.007 [2] Tian, J., Xiong, R., Shen, W., Lu, J., \& Yang, X. G. (2021). Deep neural network battery charging curve prediction using 30 points collected in 10 min. Joule, 5(6), 1521-1534. https://doi.org/10.1016/j.joule.2021.05.012 Please also consider citing our papers on these topics, see: http://en.aesa.net.cn/About.aspx?ClassID=12},
    language = {en},
    urldate = {2026-02-07},
    publisher = {Mendeley Data},
    author = {Lu, Jiahuan and Xiong, Rui and Tian, Jinpeng and Wang, Chenxu and Hsu, Chia-Wei and Tsou, Nien-Ti and Sun, Fengchun and Li, Ju},
    month = may,
    year = {2022},
    keywords = {⭐⭐⭐⭐⭐},
}
@misc{shalavi_accurate_2023,
    title = {Accurate {Calibration} of {Power} {Measurements} from {Internal} {Power} {Sensors} on {NVIDIA} {Jetson} {Devices}},
    url = {http://arxiv.org/abs/2306.13107},
    doi = {10.48550/arXiv.2306.13107},
    abstract = {Power efficiency is a crucial consideration for embedded systems design, particularly in the field of edge computing and IoT devices. This study aims to calibrate the power measurements obtained from the built-in sensors of NVIDIA Jetson devices, facilitating the collection of reliable and precise power consumption data in real-time. To achieve this goal, accurate power readings are obtained using external hardware, and a regression model is proposed to map the sensor measurements to the true power values. Our results provide insights into the accuracy and reliability of the built-in power sensors for various Jetson edge boards and highlight the importance of calibrating their internal power readings. In detail, internal sensors underestimate the actual power by up to 50\% in most cases, but this calibration reduces the error to within 3\%. By making the internal sensor data usable for precise online assessment of power and energy figures, the regression models presented in this paper have practical applications, for both practitioners and researchers, in accurately designing energy-efficient and autonomous edge services.},
    urldate = {2026-02-04},
    publisher = {arXiv},
    author = {Shalavi, Neda and Khoshsirat, Aria and Stellini, Marco and Zanella, Andrea and Rossi, Michele},
    month = jun,
    year = {2023},
    note = {arXiv:2306.13107 [eess]},
    keywords = {Domain/Hardware, Metric/accuracy, Metric/energy, Top/Edge/Orin, ⭐⭐⭐⭐⭐},
}
@misc{bai_empirical_2018,
    title = {An {Empirical} {Evaluation} of {Generic} {Convolutional} and {Recurrent} {Networks} for {Sequence} {Modeling}},
    url = {http://arxiv.org/abs/1803.01271},
    doi = {10.48550/arXiv.1803.01271},
    abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN .},
    urldate = {2026-02-07},
    publisher = {arXiv},
    author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
    month = apr,
    year = {2018},
    note = {6006 citations (Semantic Scholar/arXiv) [2026-02-07]
arXiv:1803.01271 [cs]},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}
@article{zhao_battery_2025,
    title = {Battery state of health estimation under fast charging via deep transfer learning},
    volume = {28},
    issn = {2589-0042},
    url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC12033934/},
    doi = {10.1016/j.isci.2025.112235},
    abstract = {Accurate state of health (SOH) estimation is essential for effective lithium-ion battery management, particularly under fast-charging conditions with a constrained voltage window. This study proposes a hybrid deep neural network (DNN) learning model to improve SOH prediction. With approximately 22,000 parameters, the model effectively estimates battery capacity by combining local feature extraction (convolutional neural networks [CNNs]) and global dependency analysis (self-attention). The model was validated on 222 lithium iron phosphate (LFP) batteries, encompassing 146,074 cycles, with limited data availability in a state of charge (SOC) range of 80\%–97\%. Trained on fast-charging protocols (3.6C–8C charge, 4C discharge), it demonstrates high predictive accuracy, achieving a mean absolute percentage error (MAPE) of 3.89 mAh, a root-mean-square error (RMSE) of 4.79 mAh, and a coefficient of determination (R2) of 0.97. By integrating local and global analysis, this approach significantly enhances battery aging detection under fast-charging conditions, demonstrating strong potential for battery health management systems., •Proposed a CNN and self-attention-based multi-fusion model for battery SOH estimation•Two datasets of 222 LFP batteries are used for training, validation, and testing•Achieves high accuracy using partial voltage and capacity in an end-to-end approach•Transfer learning enhances generalization to capture cell-to-cell variations, Electrochemical energy storage; Energy storage; Energy systems},
    number = {5},
    urldate = {2026-02-07},
    journal = {iScience},
    author = {Zhao, Jingyuan and Li, Di and Li, Yuqi and Shi, Dapai and Nan, Jinrui and Burke, Andrew F.},
    month = mar,
    year = {2025},
    note = {19 citations (Semantic Scholar/DOI) [2026-02-07]
TLDR: A hybrid deep neural network (DNN) learning model is proposed to improve SOH prediction and significantly enhances battery aging detection under fast-charging conditions, demonstrating strong potential for battery health management systems.},
    pages = {112235},
}
@inproceedings{sainath_convolutional_2015,
    title = {Convolutional, {Long} {Short}-{Term} {Memory}, fully connected {Deep} {Neural} {Networks}},
    issn = {2379-190X},
    url = {https://ieeexplore.ieee.org/abstract/document/7178838},
    doi = {10.1109/ICASSP.2015.7178838},
    abstract = {Both Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) have shown improvements over Deep Neural Networks (DNNs) across a wide variety of speech recognition tasks. CNNs, LSTMs and DNNs are complementary in their modeling capabilities, as CNNs are good at reducing frequency variations, LSTMs are good at temporal modeling, and DNNs are appropriate for mapping features to a more separable space. In this paper, we take advantage of the complementarity of CNNs, LSTMs and DNNs by combining them into one unified architecture. We explore the proposed architecture, which we call CLDNN, on a variety of large vocabulary tasks, varying from 200 to 2,000 hours. We find that the CLDNN provides a 4-6\% relative improvement in WER over an LSTM, the strongest of the three individual models.},
    urldate = {2026-02-07},
    booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
    author = {Sainath, Tara N. and Vinyals, Oriol and Senior, Andrew and Sak, Haşim},
    month = apr,
    year = {2015},
    note = {1484 citations (Semantic Scholar/DOI) [2026-02-07]
ISSN: 2379-190X},
    keywords = {Context, Hidden Markov models, Neural networks, Noise measurement, Speech, Speech recognition, Training},
    pages = {4580--4584},
}
@misc{noauthor_nvidia_nodate,
    title = {{NVIDIA} {TensorRT} {Documentation} — {NVIDIA} {TensorRT}},
    url = {https://docs.nvidia.com/deeplearning/tensorrt/latest/index.html},
    urldate = {2026-02-07},
    keywords = {Domain/Documentation, Domain/Edge, Sec/Edge, Sec/Modelling, Top/Edge/Orin, ⭐⭐⭐⭐⭐},
}
@misc{wang_cognitive_2025,
    title = {Cognitive {Edge} {Computing}: {A} {Comprehensive} {Survey} on {Optimizing} {Large} {Models} and {AI} {Agents} for {Pervasive} {Deployment}},
    shorttitle = {Cognitive {Edge} {Computing}},
    url = {http://arxiv.org/abs/2501.03265},
    doi = {10.48550/arXiv.2501.03265},
    abstract = {This article surveys Cognitive Edge Computing as a practical and methodical pathway for deploying reasoning-capable Large Language Models (LLMs) and autonomous AI agents on resource-constrained devices at the network edge. We present a unified, cognition-preserving framework spanning: (1) model optimization (quantization, sparsity, low-rank adaptation, distillation) aimed at retaining multi-step reasoning under tight memory/compute budgets; (2) system architecture (on-device inference, elastic offloading, cloud-edge collaboration) that trades off latency, energy, privacy, and capacity; and (3) adaptive intelligence (context compression, dynamic routing, federated personalization) that tailors computation to task difficulty and device constraints. We synthesize advances in efficient Transformer design, multimodal integration, hardware-aware compilation, privacy-preserving learning, and agentic tool use, and map them to edge-specific operating envelopes. We further outline a standardized evaluation protocol covering latency, throughput, energy per token, accuracy, robustness, privacy, and sustainability, with explicit measurement assumptions to enhance comparability. Remaining challenges include modality-aware reasoning benchmarks, transparent and reproducible energy reporting, edge-oriented safety/alignment evaluation, and multi-agent testbeds. We conclude with practitioner guidelines for cross-layer co-design of algorithms, runtime, and hardware to deliver reliable, efficient, and privacy-preserving cognitive capabilities on edge devices.},
    urldate = {2026-02-07},
    publisher = {arXiv},
    author = {Wang, Xubin and Li, Qing and Jia, Weijia},
    month = nov,
    year = {2025},
    note = {15 citations (Semantic Scholar/DOI) [2026-02-07]
arXiv:2501.03265 [cs]
TLDR: An optimization paradigm based on the data-model-system triad is proposed to enable a whole set of solutions to effectively transfer ML models, which are initially trained in the cloud, to various edge devices for supporting multiple scenarios.},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}
@article{madhavi_performance_2026,
    title = {Performance analysis of state of charge and state of health prediction using {Kalman} filter techniques with battery parameter variation},
    issn = {2096-5117},
    url = {https://www.sciencedirect.com/science/article/pii/S2096511725001410},
    doi = {10.1016/j.gloei.2025.08.004},
    abstract = {Accurate estimation of the State of Charge (SOC), State of Health (SOH), and Terminal Resistance (TR) is crucial for the effective operation of Battery Management Systems (BMS) in lithium-ion batteries. This study conducts a comprehensive comparative analysis of four Kalman filter variants Extended Kalman Filter (EKF), Extended Kalman-Bucy Filter (EKBF), Unscented Kalman Filter (UKF), and Unscented Kalman-Bucy Filter (UKBF) under varying battery parameter conditions. These include temperature fluctuation, self-discharge, current direction, cell capacity, process noise, and measurement noise. Our findings reveal significant variations in the performance of SOC and SOH predictions across filters, emphasizing that UKF demonstrates superior robustness to noise, while EKF performs better under accurate system dynamics. The study underscores the need for adaptive filtering strategies that can dynamically adjust to evolving battery parameters, thereby enhancing BMS reliability and extending battery lifespan.},
    urldate = {2026-02-07},
    journal = {Global Energy Interconnection},
    author = {Madhavi, Ranagani and Vairavasundaram, Indragandhi},
    month = jan,
    year = {2026},
    note = {0 citations (Semantic Scholar/DOI) [2026-02-07]},
    keywords = {Extended Kalman Bucy Filter, Extended Kalman Filter, State of charge, State of health, Unscented Kalman Bucy Filter, Unscented Kalman Filter},
}
@article{cui_hybrid_2022,
    title = {Hybrid {Methods} {Using} {Neural} {Network} and {Kalman} {Filter} for the {State} of {Charge} {Estimation} of {Lithium}-{Ion} {Battery}},
    volume = {2022},
    copyright = {Copyright © 2022 Zhenhua Cui et al.},
    issn = {1563-5147},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/9616124},
    doi = {10.1155/2022/9616124},
    abstract = {With the increasing carbon emissions worldwide, lithium-ion batteries have become the main component of energy storage systems for clean energy due to their unique advantages. Accurate and reliable state-of-charge (SOC) estimation is a central factor in the widespread use of lithium-ion batteries. This review, therefore, examines the recent literature on estimating the SOC of lithium-ion batteries using the hybrid methods of neural networks combined with Kalman filtering (NN-KF), classifying the methods into Kalman filter-first and neural network-first methods. Then the hybrid methods are studied and discussed in terms of battery model, parameter identification, algorithm structure, implementation process, appropriate environment, advantages, disadvantages, and estimation errors. In addition, this review also gives corresponding recommendations for researchers in the battery field considering the existing problems.},
    language = {en},
    number = {1},
    urldate = {2026-02-07},
    journal = {Mathematical Problems in Engineering},
    author = {Cui, Zhenhua and Dai, Jiyong and Sun, Jianrui and Li, Dezhi and Wang, Licheng and Wang, Kai},
    year = {2022},
    note = {76 citations (Semantic Scholar/DOI) [2026-02-07]
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1155/2022/9616124
TLDR: This review examines the recent literature on estimating the SOC of lithium-ion batteries using the hybrid methods of neural networks combined with Kalman filtering (NN-KF), classifying the methods into Kalman filter-first and neural network-first methods.},
    pages = {9616124},
}
@article{yu_state_2023,
    title = {State of charge estimation method by using a simplified electrochemical model in deep learning framework for lithium-ion batteries},
    volume = {278},
    issn = {0360-5442},
    url = {https://www.sciencedirect.com/science/article/pii/S0360544223012409},
    doi = {10.1016/j.energy.2023.127846},
    abstract = {To ensure the secure and healthy usage of lithium-ion batteries, it is necessary to accurately estimate the state of charge (SOC) in battery management systems. The development of deep learning (DL) provides a new solution for battery SOC estimation. However, the directly measured physical quantities contain less useful information and have low estimation accuracy. This paper proposes a method of integrating the mechanism knowledge of the battery domain into the DL framework. Firstly, the simplified electrochemical model is utilized to obtain the mechanism-related physical variables to expand the input of the DL model. Secondly, the long short-term memory (LSTM) network is used with the Bayesian optimization, and the variables with high correlation are identified. The best SOC estimation performance can be obtained by adding all the selected highly-correlated variables to the input for training together. The results show that the proposed method can improve the SOC estimation performance with only a slight increase in computation cost. Finally, other DL models are utilized to further validate the effectiveness, to reveal the universality. These results show that the performance of the DL model can be effectively improved by using the knowledge of the battery domain.},
    urldate = {2026-02-04},
    journal = {Energy},
    author = {Yu, Hanqing and Zhang, Lisheng and Wang, Wentao and Li, Shen and Chen, Siyan and Yang, Shichun and Li, Junfu and Liu, Xinhua},
    month = sep,
    year = {2023},
    note = {70 citations (Crossref/DOI) [2026-02-07]
65 citations (Semantic Scholar/DOI) [2026-02-04]},
    keywords = {Alg/hybrid, Alg/ml/LSTM, Data/Lab, Domain/SOC, Metric/accuracy, Sec/Background, Sec/Modelling, Top/PC, ⭐⭐⭐⭐},
    pages = {127846},
}